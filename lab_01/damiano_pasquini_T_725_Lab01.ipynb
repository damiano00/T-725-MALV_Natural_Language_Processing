{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "cA4f3BP_gZUC"
   },
   "source": [
    "# T-725 Natural Language Processing: Lab 1\n",
    "In these labs, we will be using the [Python 3](https://www.python.org/) programming language and the [Natural Language Toolkit (NLTK)](https://www.nltk.org/). We will also be using Google Colab, a free service hosted by Google, which gives us access to a Linux machine that comes pre-installed with Python 3 and the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThiaruImU8ZV",
    "outputId": "2dc6d2d8-3c62-4148-9c5e-63e79fc439f4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMY8d4CJ46K2"
   },
   "source": [
    "## Using Google Colab\n",
    "Google Colab allows users to work with \"notebooks\", which consists of text cells and code cells. Text cells can be edited by double clicking them. A code cell can be executed by selecting it and pressing `Ctrl + Enter`. Code is shared between cells, meaning that you can, for example, create a variable in one cell and use it in another cell later on.\n",
    "\n",
    "To begin with, do the following:\n",
    "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
    "* Select `\"Runtime\" > \"Run all\"` to run all of the code cells in the notebook.\n",
    "\n",
    "## Resources\n",
    "* [The Python Standard Library](https://docs.python.org/3/library/index.html) - an overview of the built-in libraries in Python with a lot of examples.\n",
    "* [The Python Tutorial](https://docs.python.org/3/tutorial/index.html) - an official tutorial that gives a brief overview of the language.\n",
    "* [Automate the Boring Stuff with Python](https://automatetheboringstuff.com/) - a free book that offers a good introduction to the Python programming language to beginners.\n",
    "* [Natural Language Processing with Python](http://www.nltk.org/book/) - a free companion book to the NLTK toolkit.\n",
    "\n",
    "## Setting Python and the NLTK up on your own machine\n",
    "* [Python 3](https://realpython.com/installing-python/) - installation instructions\n",
    "* [NLTK](https://www.nltk.org/install.html) - installation instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f19uyslsaYw4"
   },
   "source": [
    "## String methods in Python\n",
    "There are many ways to manipulate strings in Python. A full list of methods for the String class may be found in the [library reference](https://docs.python.org/3/library/stdtypes.html#string-methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzTEm3RCgZUU",
    "outputId": "c5d791ff-e09f-494f-ebf8-24c347f04b1d"
   },
   "outputs": [],
   "source": [
    "a_string = \"It was the best of times, it was the worst of times\"\n",
    "\n",
    "print(\"Lowercase:\", a_string.lower())\n",
    "print(\"'times' count:\", a_string.count('times'))\n",
    "print(\"First occurence of 'best':\", a_string.find('best'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmTHYN-agZVa"
   },
   "source": [
    "## Lists, sets and built-in functions\n",
    "Lists and sets are two kinds of collections that can be used in Python.\n",
    "* A **list** is an *ordered* sequence of elements. Lists are enclosed with square brackets, and the elements are separated with a comma (e.g., `a_list = [\"This\", \"is\", \"a\", \"list\"]`).\n",
    "* A **set** is a collection of *unordered* and *unique* elements (meaning that it contains no duplicates). Sets are enclosed by curly braces, and the elements are separated by commas (e.g., `a_set = {\"This\", \"is\", \"a\", \"set\"}`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8NWJM2rGgZVf",
    "outputId": "ed6848f0-107a-4f74-8bd2-9f515d0b3152",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Variables can be converted to lists and sets with the list() and set() functions\n",
    "char_list = list(a_string)\n",
    "char_set = set(a_string)\n",
    "\n",
    "# You can also split strings on certain characters to create a list of strings\n",
    "words = a_string.split()  # Splits on whitespaces by default\n",
    "print(\"Split string:\", words)\n",
    "\n",
    "# The len() and max() built-in methods\n",
    "print(\"Unique characters:\", char_set)\n",
    "print(\"No. of unique characters:\", len(char_set))\n",
    "print(\"Longest word:\", max(words, key=len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUt68ulEpfuw"
   },
   "source": [
    "You can use the built-in function `help()` to quickly access documentation for a given object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "En1YvSC4qHJ8",
    "outputId": "6cd0a61a-1947-43b6-f909-6731456154b2"
   },
   "outputs": [],
   "source": [
    "help(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X_TWoEUgZVw"
   },
   "source": [
    "## Indices and slicing\n",
    "You can get characters and substrings at specific indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_p4o5Lc_gZV2",
    "outputId": "02afd37e-1cb2-4a0b-ea87-00e72300f707"
   },
   "outputs": [],
   "source": [
    "print(\"String:\", a_string)\n",
    "print(\"First character:\", a_string[0])  # Indices in Python start at 0\n",
    "print(\"Last character:\", a_string[-1])  # A negative index starts counting from the end\n",
    "\n",
    "# We can get ranges of elements by slicing a list\n",
    "print(\"Characters 11 to 14:\", a_string[11:15])\n",
    "print(\"First 6 characters:\", a_string[:6])\n",
    "print(\"Last 5 characters:\", a_string[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wQ9qvZ2gZV_"
   },
   "source": [
    "## The Natural Language Toolkit (NLTK)\n",
    "The NLTK book, [Natural Language Processing with Python](http://www.nltk.org/book/), is an introduction to natural language processing in Python, using the NLTK library. [Chapter 1](http://www.nltk.org/book/ch01.html) is relevant to this lab. The NLTK comes with a lot of data, such as corpora and trained models. We can download this data with the `nltk.download()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txGWuokGgZWF",
    "outputId": "c3cb162f-3044-4267-d31b-c5c7e5483ae8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Download the 'gutenberg' corpus, which is a collection of books in the public domain\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# Get a plain-text version of Moby Dick (contained in a single string)\n",
    "moby_raw = gutenberg.raw('melville-moby_dick.txt')\n",
    "\n",
    "# Get all the tokens in Moby Dick (as a list of strings)\n",
    "moby_tokens = gutenberg.words('melville-moby_dick.txt')\n",
    "\n",
    "# Print the first 10 tokens of Moby Dick\n",
    "print(\"First 10 tokens:\", moby_tokens[:10])\n",
    "\n",
    "# Print the first 250 characters of Moby Dick\n",
    "print(\"\\nFirst 250 characters:\\n\", moby_raw[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk7r17iXgZW-"
   },
   "source": [
    "NLTK includes a Text class for analyzing the contents of texts. Let's print a concordance for the word *Iceland*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCPcW0p9gZXB",
    "outputId": "7175cc3b-d272-4d26-c323-285ed3efa350"
   },
   "outputs": [],
   "source": [
    "moby_text = nltk.Text(moby_tokens)\n",
    "moby_text.concordance('Iceland')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntDr5ACsgZXM"
   },
   "source": [
    "NTLK offers several ways to segment text into sentences and tokenize it. Let's see how `nltk.sent_tokenize()` and `nltk.word_tokenize()` work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8IZ1nLySgZXQ",
    "outputId": "8a8db844-f3f0-464b-8f94-b12b190948c2"
   },
   "outputs": [],
   "source": [
    "# Download the Punkt tokenizer model\n",
    "nltk.download('punkt')\n",
    "\n",
    "moby_sentences = nltk.sent_tokenize(moby_raw)  # Split raw text into sentences\n",
    "tokens = nltk.word_tokenize(moby_sentences[3])  # Split a string into tokens\n",
    "\n",
    "print(\"First 5 sentences:\")\n",
    "for sentence in moby_sentences[:5]:\n",
    "    print(\">>>\", sentence)\n",
    "\n",
    "print(f\"\\nTotal number of sentences: {len(moby_sentences):,}\")\n",
    "\n",
    "print(\"\\nTokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC_OW-lUgZXo"
   },
   "source": [
    "## Regular expressions\n",
    "The Python standard library includes the `re` module for handling regular expressions ([reference](https://docs.python.org/3/library/re.html)). In Python, regular expression patterns should be created using *raw* strings, which are prefixed with an `r`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7cDMt90gZXs",
    "outputId": "2a8dd407-6dd3-4412-b007-b884210386d1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(r'\\b\\S{9,}est\\b', moby_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0z_417IcgZX1"
   },
   "source": [
    "You can capture text that matches a specific part of a pattern in a group by enclosing it within parentheses. When making substitutions with `re.sub()`, you can refer to these groups using `\\number` (e.g., `\\1` and `\\2`), where the number refers to their position in the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "aHyZVF5pgZX3",
    "outputId": "20617e73-3d62-4258-955e-0961ee51e7a5"
   },
   "outputs": [],
   "source": [
    "another_string = \"The grapes of wrath\"\n",
    "re.sub(r'(\\S+) of (\\S+)', r'\\2 of \\1', another_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-U35BXYgZYD"
   },
   "source": [
    "NLTK offers a simple way of searching for sequences of tokens using regular expressions, where tokens can be enclosed in angle brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRChhPklgZYH",
    "outputId": "a94d1b76-a1a3-4302-b26d-8c127ebcec01"
   },
   "outputs": [],
   "source": [
    "# First we must create a Text object from a list of tokens\n",
    "moby_text = nltk.Text(moby_tokens)\n",
    "\n",
    "# Let's search for sequences of four tokens which all begin with the letter S\n",
    "moby_text.findall(r'<[Ss].*>{4,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIt3E060gZYQ"
   },
   "source": [
    "You can use groups to target specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3L3nrg2SgZYV",
    "outputId": "cee48dc8-a9b9-411e-9b7d-c96bd6322751",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moby_text.findall(r'<[Aa]n?>(<.+>)<ship>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ksz_n_cwgZYb"
   },
   "source": [
    "# Assignment\n",
    "Answer the following questions and hand in your solution in Canvas before midnight tonight, September 1st. Make a copy of this notebook (`\"File\" > \"Save a copy in Drive\"`) and enter your solutions in the cells below each question. Remember to save your file before uploading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEcJJbIUgZYi"
   },
   "source": [
    "### Question 1\n",
    "Get the raw text of `carroll-alice.txt` (Alice in Wonderland) from the Gutenberg corpus in NLTK and tokenize it using `nltk.word_tokenize()`.\n",
    "\n",
    "1. How many tokens does it contain in total?\n",
    "2. How many unique tokens does it contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dsA5lgp0gZYx",
    "outputId": "ada7f21f-e6cc-49e6-8e10-06aad2b05335"
   },
   "outputs": [],
   "source": [
    "# answer: carrol-alice.txt contains 33494 tokens\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(gutenberg.raw('carroll-alice.txt'))\n",
    "print(\"Total number of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M9OMFNxgZY6"
   },
   "source": [
    "### Question 2\n",
    "Use `nltk.FreqDist()` to create a frequency distribution of all the tokens in Alice in Wonderland. What are the 20 most frequently occurring tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOPA_dljgZY_",
    "outputId": "a6c7f1ef-524c-4c14-efbc-9c4f0e0d0d73"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "import nltk\n",
    "nltk.FreqDist(tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ku3fBv6gZZH"
   },
   "source": [
    "### Question 3\n",
    "Use `nltk.sent_tokenize()` to segment Alice in Wonderland into sentences, then find the longest sentence in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EuYbFqJgZZI",
    "outputId": "c3273bde-46fe-494f-abc0-0932e8943b3e"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "sentences = nltk.sent_tokenize(gutenberg.raw('carroll-alice.txt'))\n",
    "longest = max(sentences, key=len)\n",
    "print(\"Longest sentence:\", longest)\n",
    "print(\"\\nLength:\", len(longest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExlDi0jDgZZR"
   },
   "source": [
    "### Question 4\n",
    "Use a regular expression to find all tokens in Alice in Wonderland that contain an *x* and end with *ed*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtfpRxu5gZZT"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "import re\n",
    "\n",
    "tokens_2 = re.findall(r'\\w+x\\w*ed\\b', gutenberg.raw('carroll-alice.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0rG5i_VgZZa"
   },
   "source": [
    "### Question 5\n",
    "Use `re.sub()` to \"dehyphenate\" the following string:\n",
    "\n",
    ">It is a capital mistake to theo-  \n",
    ">rize before one has data. Insen-  \n",
    ">sibly one begins to twist facts  \n",
    ">to suit theories, instead of the-  \n",
    ">ories to suit facts.\n",
    "\n",
    "You will need to use groups to recombine the words. The resulting string should look like this:\n",
    "\n",
    ">It is a capital mistake to  \n",
    ">theorize before one has data.  \n",
    ">Insensibly one begins to twist facts  \n",
    ">to suit theories, instead of  \n",
    ">theories to suit facts.\n",
    "\n",
    "Remember that a \"newline\" character is represented by `\\n` in strings and regular expression patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKyWgOPkgZZe",
    "outputId": "20653b25-1224-4060-bfde-344fca9dc696"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "hyphenated = \"\"\"\n",
    "It is a capital mistake to theo-\n",
    "rize before one has data. Insen-\n",
    "sibly one begins to twist facts\n",
    "to suit theories, instead of the-\n",
    "ories to suit facts.\n",
    "\"\"\"\n",
    "\n",
    "dehyphenated = re.sub(r'(\\w+)-\\n(\\w+)', r'\\n\\1\\2', hyphenated)\n",
    "print(dehyphenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-6gf_nZjY8p"
   },
   "source": [
    "## UNIX Text Processing Tools\n",
    "Solve the following questions using UNIX tools for text processing. Use the resources in the \"Text Processing Tools\" module on Canvas for this part of the lab.\n",
    "\n",
    "Download the 'lab1.txt' on the Testfiles page in that module.\n",
    "\n",
    "To run UNIX commands in CoLab, start your line with an !\n",
    "\n",
    "Upload the necessary files into CoLab by clicking the folder icon in the left side navigation bar and then the icon that has a document with an up-arrow on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-XAl2EjjyZy"
   },
   "source": [
    "###Question 6\n",
    "\n",
    "Use egrep to match all lines that make reference to a decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UP5Hrfv7Uq8Y",
    "outputId": "aceedd6c-2da9-4cb5-b1fe-d8bf7412ad42"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "!egrep [1-9]{{3}}0's\\b' lab1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EweJXuTrxmR"
   },
   "source": [
    "###Question 7\n",
    "\n",
    "Use sed to replace all occurrences of years or decades with the string \"CLASSIFIED\". Note, special symbols like + and ? must be escaped using the \\ character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSSqTJ57rxTZ",
    "outputId": "e32a21b3-7a89-48d5-c67b-62e34681eac4"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "!sed -E 's/[0-9]{{4}}s?/CLASSIFIED/g' lab1.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXTyF67qwBd9"
   },
   "source": [
    "###Question 8\n",
    "\n",
    "Translate all uppercase letters to lowercase using `tr '[:upper:]' '[:lower:]' < inputfile`\n",
    "\n",
    "Then use `sed` to replace all non-alphanumeric characters with a newline, and remove all empty lines. Store the output in a file called `tokens.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJg_x59iwDrI"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "!tr '[:upper:]' '[:lower:]' < lab1.txt | sed -E 's/[^a-z0-9]/\\n/g' | sed '/^\\s*$/d' > tokens.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5e5o0SavHzf"
   },
   "source": [
    "### Question 9\n",
    "\n",
    "Create a unigram model of `tokens.txt` using UNIX commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vs1WouS3vZ02"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "!cat tokens.txt | sort | uniq -c | sort -nr > unigram.txt"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
