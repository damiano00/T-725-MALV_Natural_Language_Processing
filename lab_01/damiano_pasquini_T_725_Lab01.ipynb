{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "cA4f3BP_gZUC"
   },
   "source": [
    "# T-725 Natural Language Processing: Lab 1\n",
    "In these labs, we will be using the [Python 3](https://www.python.org/) programming language and the [Natural Language Toolkit (NLTK)](https://www.nltk.org/). We will also be using Google Colab, a free service hosted by Google, which gives us access to a Linux machine that comes pre-installed with Python 3 and the NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMY8d4CJ46K2"
   },
   "source": [
    "## Using Google Colab\n",
    "Google Colab allows users to work with \"notebooks\", which consists of text cells and code cells. Text cells can be edited by double clicking them. A code cell can be executed by selecting it and pressing `Ctrl + Enter`. Code is shared between cells, meaning that you can, for example, create a variable in one cell and use it in another cell later on.\n",
    "\n",
    "To begin with, do the following:\n",
    "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
    "* Select `\"Runtime\" > \"Run all\"` to run all of the code cells in the notebook.\n",
    "\n",
    "## Resources\n",
    "* [The Python Standard Library](https://docs.python.org/3/library/index.html) - an overview of the built-in libraries in Python with a lot of examples.\n",
    "* [The Python Tutorial](https://docs.python.org/3/tutorial/index.html) - an official tutorial that gives a brief overview of the language.\n",
    "* [Automate the Boring Stuff with Python](https://automatetheboringstuff.com/) - a free book that offers a good introduction to the Python programming language to beginners.\n",
    "* [Natural Language Processing with Python](http://www.nltk.org/book/) - a free companion book to the NLTK toolkit.\n",
    "\n",
    "## Setting Python and the NLTK up on your own machine\n",
    "* [Python 3](https://realpython.com/installing-python/) - installation instructions\n",
    "* [NLTK](https://www.nltk.org/install.html) - installation instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f19uyslsaYw4"
   },
   "source": [
    "## String methods in Python\n",
    "There are many ways to manipulate strings in Python. A full list of methods for the String class may be found in the [library reference](https://docs.python.org/3/library/stdtypes.html#string-methods)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nzTEm3RCgZUU"
   },
   "source": [
    "a_string = \"It was the best of times, it was the worst of times\"\n",
    "\n",
    "print(\"Lowercase:\", a_string.lower())\n",
    "print(\"'times' count:\", a_string.count('times'))\n",
    "print(\"First occurence of 'best':\", a_string.find('best'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmTHYN-agZVa"
   },
   "source": [
    "## Lists, sets and built-in functions\n",
    "Lists and sets are two kinds of collections that can be used in Python.\n",
    "* A **list** is an *ordered* sequence of elements. Lists are enclosed with square brackets, and the elements are separated with a comma (e.g., `a_list = [\"This\", \"is\", \"a\", \"list\"]`).\n",
    "* A **set** is a collection of *unordered* and *unique* elements (meaning that it contains no duplicates). Sets are enclosed by curly braces, and the elements are separated by commas (e.g., `a_set = {\"This\", \"is\", \"a\", \"set\"}`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "8NWJM2rGgZVf"
   },
   "source": [
    "# Variables can be converted to lists and sets with the list() and set() functions\n",
    "char_list = list(a_string)\n",
    "char_set = set(a_string)\n",
    "\n",
    "# You can also split strings on certain characters to create a list of strings\n",
    "words = a_string.split()  # Splits on whitespaces by default\n",
    "print(\"Split string:\", words)\n",
    "\n",
    "# The len() and max() built-in methods\n",
    "print(\"Unique characters:\", char_set)\n",
    "print(\"No. of unique characters:\", len(char_set))\n",
    "print(\"Longest word:\", max(words, key=len))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUt68ulEpfuw"
   },
   "source": [
    "You can use the built-in function `help()` to quickly access documentation for a given object."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "En1YvSC4qHJ8"
   },
   "source": [
    "help(max)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X_TWoEUgZVw"
   },
   "source": [
    "## Indices and slicing\n",
    "You can get characters and substrings at specific indexes:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_p4o5Lc_gZV2"
   },
   "source": [
    "print(\"String:\", a_string)\n",
    "print(\"First character:\", a_string[0])  # Indices in Python start at 0\n",
    "print(\"Last character:\", a_string[-1])  # A negative index starts counting from the end\n",
    "\n",
    "# We can get ranges of elements by slicing a list\n",
    "print(\"Characters 11 to 14:\", a_string[11:15])\n",
    "print(\"First 6 characters:\", a_string[:6])\n",
    "print(\"Last 5 characters:\", a_string[-5:])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wQ9qvZ2gZV_"
   },
   "source": [
    "## The Natural Language Toolkit (NLTK)\n",
    "The NLTK book, [Natural Language Processing with Python](http://www.nltk.org/book/), is an introduction to natural language processing in Python, using the NLTK library. [Chapter 1](http://www.nltk.org/book/ch01.html) is relevant to this lab. The NLTK comes with a lot of data, such as corpora and trained models. We can download this data with the `nltk.download()` function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "txGWuokGgZWF"
   },
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Download the 'gutenberg' corpus, which is a collection of books in the public domain\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# Get a plain-text version of Moby Dick (contained in a single string)\n",
    "moby_raw = gutenberg.raw('melville-moby_dick.txt')\n",
    "\n",
    "# Get all the tokens in Moby Dick (as a list of strings)\n",
    "moby_tokens = gutenberg.words('melville-moby_dick.txt')\n",
    "\n",
    "# Print the first 10 tokens of Moby Dick\n",
    "print(\"First 10 tokens:\", moby_tokens[:10])\n",
    "\n",
    "# Print the first 250 characters of Moby Dick\n",
    "print(\"\\nFirst 250 characters:\\n\", moby_raw[:250])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk7r17iXgZW-"
   },
   "source": [
    "NLTK includes a Text class for analyzing the contents of texts. Let's print a concordance for the word *Iceland*:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YCPcW0p9gZXB"
   },
   "source": [
    "moby_text = nltk.Text(moby_tokens)\n",
    "moby_text.concordance('Iceland')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntDr5ACsgZXM"
   },
   "source": [
    "NTLK offers several ways to segment text into sentences and tokenize it. Let's see how `nltk.sent_tokenize()` and `nltk.word_tokenize()` work:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8IZ1nLySgZXQ"
   },
   "source": [
    "# Download the Punkt tokenizer model\n",
    "nltk.download('punkt')\n",
    "\n",
    "moby_sentences = nltk.sent_tokenize(moby_raw)  # Split raw text into sentences\n",
    "tokens = nltk.word_tokenize(moby_sentences[3])  # Split a string into tokens\n",
    "\n",
    "print(\"First 5 sentences:\")\n",
    "for sentence in moby_sentences[:5]:\n",
    "    print(\">>>\", sentence)\n",
    "\n",
    "print(f\"\\nTotal number of sentences: {len(moby_sentences):,}\")\n",
    "\n",
    "print(\"\\nTokens:\", tokens)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC_OW-lUgZXo"
   },
   "source": [
    "## Regular expressions\n",
    "The Python standard library includes the `re` module for handling regular expressions ([reference](https://docs.python.org/3/library/re.html)). In Python, regular expression patterns should be created using *raw* strings, which are prefixed with an `r`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "id": "I7cDMt90gZXs"
   },
   "source": [
    "import re\n",
    "re.findall(r'\\b\\S{9,}est\\b', moby_raw)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0z_417IcgZX1"
   },
   "source": [
    "You can capture text that matches a specific part of a pattern in a group by enclosing it within parentheses. When making substitutions with `re.sub()`, you can refer to these groups using `\\number` (e.g., `\\1` and `\\2`), where the number refers to their position in the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aHyZVF5pgZX3"
   },
   "source": [
    "another_string = \"The grapes of wrath\"\n",
    "re.sub(r'(\\S+) of (\\S+)', r'\\2 of \\1', another_string)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-U35BXYgZYD"
   },
   "source": [
    "NLTK offers a simple way of searching for sequences of tokens using regular expressions, where tokens can be enclosed in angle brackets:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "id": "pRChhPklgZYH"
   },
   "source": [
    "# First we must create a Text object from a list of tokens\n",
    "moby_text = nltk.Text(moby_tokens)\n",
    "\n",
    "# Let's search for sequences of four tokens which all begin with the letter S\n",
    "moby_text.findall(r'<[Ss].*>{4,}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIt3E060gZYQ"
   },
   "source": [
    "You can use groups to target specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "3L3nrg2SgZYV"
   },
   "source": [
    "moby_text.findall(r'<[Aa]n?>(<.+>)<ship>')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ksz_n_cwgZYb"
   },
   "source": [
    "# Assignment\n",
    "Answer the following questions and hand in your solution in Canvas before midnight tonight, September 1st. Make a copy of this notebook (`\"File\" > \"Save a copy in Drive\"`) and enter your solutions in the cells below each question. Remember to save your file before uploading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEcJJbIUgZYi"
   },
   "source": [
    "### Question 1\n",
    "Get the raw text of `carroll-alice.txt` (Alice in Wonderland) from the Gutenberg corpus in NLTK and tokenize it using `nltk.word_tokenize()`.\n",
    "\n",
    "1. How many tokens does it contain in total?\n",
    "2. How many unique tokens does it contain?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "id": "dsA5lgp0gZYx"
   },
   "source": [
    "# answer: carrol-alice.txt contains 33494 tokens\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(gutenberg.raw('carroll-alice.txt'))\n",
    "print(\"Total number of tokens:\", len(tokens))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M9OMFNxgZY6"
   },
   "source": [
    "### Question 2\n",
    "Use `nltk.FreqDist()` to create a frequency distribution of all the tokens in Alice in Wonderland. What are the 20 most frequently occurring tokens?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "id": "tOPA_dljgZY_"
   },
   "source": [
    "# Your solution here\n",
    "import nltk\n",
    "nltk.FreqDist(tokens).most_common(20)\n",
    "\n",
    "\"\"\"\n",
    "SOLUTION:\n",
    "\n",
    "[(',', 2418),\n",
    " ('the', 1516),\n",
    " (\"'\", 1309),\n",
    " ('.', 975),\n",
    " ('and', 757),\n",
    " ('to', 717),\n",
    " ('a', 614),\n",
    " ('I', 533),\n",
    " ('it', 512),\n",
    " ('she', 506),\n",
    " ('of', 496),\n",
    " ('said', 456),\n",
    " ('!', 450),\n",
    " ('Alice', 394),\n",
    " ('was', 361),\n",
    " ('in', 352),\n",
    " ('you', 334),\n",
    " ('that', 267),\n",
    " ('--', 264),\n",
    " ('her', 243)]\n",
    "\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ku3fBv6gZZH"
   },
   "source": [
    "### Question 3\n",
    "Use `nltk.sent_tokenize()` to segment Alice in Wonderland into sentences, then find the longest sentence in the book."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "id": "-EuYbFqJgZZI"
   },
   "source": [
    "# Your solution here\n",
    "sentences = nltk.sent_tokenize(gutenberg.raw('carroll-alice.txt'))\n",
    "longest = max(sentences, key=len)\n",
    "print(\"Longest sentence:\", longest)\n",
    "print(\"Length:\", len(longest))\n",
    "\n",
    "\"\"\"\n",
    "SOLUTION:\n",
    "\n",
    "Longest sentence: Hardly knowing what she did, she picked up a little bit of stick, and\n",
    "held it out to the puppy; whereupon the puppy jumped into the air off\n",
    "all its feet at once, with a yelp of delight, and rushed at the stick,\n",
    "and made believe to worry it; then Alice dodged behind a great thistle,\n",
    "to keep herself from being run over; and the moment she appeared on the\n",
    "other side, the puppy made another rush at the stick, and tumbled head\n",
    "over heels in its hurry to get hold of it; then Alice, thinking it was\n",
    "very like having a game of play with a cart-horse, and expecting every\n",
    "moment to be trampled under its feet, ran round the thistle again; then\n",
    "the puppy began a series of short charges at the stick, running a very\n",
    "little way forwards each time and a long way back, and barking hoarsely\n",
    "all the while, till at last it sat down a good way off, panting, with\n",
    "its tongue hanging out of its mouth, and its great eyes half shut.\n",
    "\n",
    "Length: 919\n",
    "\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExlDi0jDgZZR"
   },
   "source": [
    "### Question 4\n",
    "Use a regular expression to find all tokens in Alice in Wonderland that contain an *x* and end with *ed*."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TtfpRxu5gZZT"
   },
   "source": [
    "# Your solution here\n",
    "import re\n",
    "\n",
    "tokens_2 = re.findall(r'\\w+x\\w+ed\\b', gutenberg.raw('carroll-alice.txt'))\n",
    "for word in tokens_2:\n",
    "    if tokens.__contains__(word):\n",
    "        continue\n",
    "    else:\n",
    "        print(word, 'not contained')        \n",
    "print(tokens_2)\n",
    "\n",
    "\"\"\"\n",
    "SOLUTION:\n",
    "\n",
    "['executed', 'expected', 'exclaimed', 'executed', 'executed', 'exclaimed', 'exclaimed', 'exclaimed', 'explained', 'exclaimed', 'executed', 'executed', 'executed', 'exclaimed']\n",
    "\n",
    "\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0rG5i_VgZZa"
   },
   "source": [
    "### Question 5\n",
    "Use `re.sub()` to \"dehyphenate\" the following string:\n",
    "\n",
    ">It is a capital mistake to theo-  \n",
    ">rize before one has data. Insen-  \n",
    ">sibly one begins to twist facts  \n",
    ">to suit theories, instead of the-  \n",
    ">ories to suit facts.\n",
    "\n",
    "You will need to use groups to recombine the words. The resulting string should look like this:\n",
    "\n",
    ">It is a capital mistake to  \n",
    ">theorize before one has data.  \n",
    ">Insensibly one begins to twist facts  \n",
    ">to suit theories, instead of  \n",
    ">theories to suit facts.\n",
    "\n",
    "Remember that a \"newline\" character is represented by `\\n` in strings and regular expression patterns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "id": "zKyWgOPkgZZe"
   },
   "source": [
    "# Your solution here\n",
    "hyphenated = \"\"\"\n",
    "It is a capital mistake to theo-\n",
    "rize before one has data. Insen-\n",
    "sibly one begins to twist facts\n",
    "to suit theories, instead of the-\n",
    "ories to suit facts.\n",
    "\"\"\"\n",
    "\n",
    "dehyphenated = re.sub(r'(\\w+)-\\n(\\w+)', r'\\n\\1\\2', hyphenated)\n",
    "print(dehyphenated)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-6gf_nZjY8p"
   },
   "source": [
    "## UNIX Text Processing Tools\n",
    "Solve the following questions using UNIX tools for text processing. Use the resources in the \"Text Processing Tools\" module on Canvas for this part of the lab.\n",
    "\n",
    "Download the 'lab1.txt' on the Testfiles page in that module.\n",
    "\n",
    "To run UNIX commands in CoLab, start your line with an !\n",
    "\n",
    "Upload the necessary files into CoLab by clicking the folder icon in the left side navigation bar and then the icon that has a document with an up-arrow on it."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Question 6\n",
    "\n",
    "Use egrep to match all lines that make reference to a decade."
   ],
   "metadata": {
    "id": "R-XAl2EjjyZy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "command_1 = \"egrep '19[1-9]0{2}' lab1.txt\"\n",
    "\n",
    "\"\"\"\n",
    "SOLUTION:\n",
    "\n",
    "command: egrep '[0-9]s' lab1.txt\n",
    "\n",
    "- unix result:\n",
    "\n",
    "The history of NLP generally starts in the 1950s, although work can be found from earlier periods.\n",
    "Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n",
    "Up to the 1980s, most NLP systems were based on complex sets of hand-written rules.\n",
    "Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing.\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Question 7\n",
    "\n",
    "Use sed to replace all occurrences of years or decades with the string \"CLASSIFIED\". Note, special symbols like + and ? must be escaped using the \\ character."
   ],
   "metadata": {
    "id": "7EweJXuTrxmR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your solution here\n",
    "\n",
    "command_2 = \"sed -E 's/[0-9]{4}/CLASSIFIED/g' lab1.txt\"\n",
    "\n",
    "\"\"\"\n",
    "SOLUTION:\n",
    "\n",
    "command: sed -E 's/[0-9]{4}/CLASSIFIED/g' lab1.txt\n",
    "\n",
    "- unix result:\n",
    "\n",
    "The history of NLP generally starts in the CLASSIFIEDs, although work can be found from earlier periods.\n",
    "In CLASSIFIED, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n",
    "\n",
    "The Georgetown experiment in CLASSIFIED involved fully automatic translation of more than sixty Russian sentences into English.\n",
    "The authors claimed that within three or five years, machine translation would be a solved problem.\n",
    "However, real progress was much slower, and after the ALPAC report in CLASSIFIED, which found that ten years long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.\n",
    "Little further research in machine translation was conducted until the late CLASSIFIEDs, when the first statistical machine translation systems were developed.\n",
    "\n",
    "Up to the CLASSIFIEDs, most NLP systems were based on complex sets of hand-written rules.\n",
    "Starting in the late CLASSIFIEDs, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing.\n",
    "This was due both to the steady increase in computational power resulting from Moore's Law and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.\n",
    "\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "id": "rSSqTJ57rxTZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Question 8\n",
    "\n",
    "Translate all uppercase letters to lowercase using `tr '[:upper:]' '[:lower:]' < inputfile`\n",
    "\n",
    "Then use `sed` to replace all non-alphanumeric characters with a newline, and remove all empty lines. Store the output in a file called `tokens.txt`."
   ],
   "metadata": {
    "id": "iXTyF67qwBd9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your solution here\n",
    "\n",
    "command_3 = \"tr '[:upper:]' '[:lower:]' < lab1.txt | sed -E 's/[^a-z0-9]/\\n/g' | sed '/^$/d' > tokens.txt\""
   ],
   "metadata": {
    "id": "KJg_x59iwDrI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 9\n",
    "\n",
    "Create a unigram model of `tokens.txt` using UNIX commands."
   ],
   "metadata": {
    "id": "A5e5o0SavHzf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your solution here\n",
    "\n",
    "command_4 = \"cat tokens.txt | sort | uniq -c | sort -nr > unigram.txt\""
   ],
   "metadata": {
    "id": "vs1WouS3vZ02"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
