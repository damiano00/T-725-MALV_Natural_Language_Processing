{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3IOeFoye2P8"
   },
   "source": [
    "# T-725 Natural Language Processing: Lab 5\n",
    "In today's lab, we will be working with neural networks, using GRUs and Transformers for text generation.\n",
    "\n",
    "To begin with, do the following:\n",
    "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
    "* **Select `\"Runtime\" > \"Change runtime type\"`, and make sure that you have \"Hardware accelerator\" set to \"GPU\"**\n",
    "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R7ElxTOtl6UQ",
    "ExecuteTime": {
     "end_time": "2023-09-29T13:24:44.571082500Z",
     "start_time": "2023-09-29T13:24:44.568076200Z"
    }
   },
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress some warnings from TensorFlow about deprecated functions\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ayz3HiU7JvCF"
   },
   "source": [
    "## Generating text with neural networks\n",
    "Let's create a neural language model and use it to generate some text. This time, we will use character embeddings rather than word embeddings. They are created in exactly the same way, and are often used together in neural network-based models. One benefit of using character embeddings is that we can generate words that our model has never seen before.\n",
    "\n",
    "The model takes as input a sequence of characters and predicts which character is most likely to follow. We will generate text by repeatedly predicting and appending the next character to a string. First, however, we need some text to train it on.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PN7I_djD91Py",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:22:28.077569400Z",
     "start_time": "2023-09-29T10:22:24.342052700Z"
    }
   },
   "source": [
    "# Based on the following tutorial:\n",
    "# https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Let's download some text by Shakespeare to train our model\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)\n",
    "\n",
    "with open(path_to_file, encoding='utf-8') as f:\n",
    "  shakespeare = f.read()\n",
    "\n",
    "print(\"First 250 characters:\")\n",
    "print(shakespeare[:250])\n",
    "\n",
    "print (\"Length of text: {:,} characters\".format(len(shakespeare)))"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n",
      "First 250 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "Length of text: 1,115,394 characters\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45JqVDxqtf1o"
   },
   "source": [
    "Now we can create training examples for our model. Each example will be a pair of strings: one input string containing 100 characters, and a target string that is one character ahead. For example, the first pair we create is:\n",
    "\n",
    "**Input string**:  `'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'`\n",
    "\n",
    "**Target string**: `'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '`\n",
    "\n",
    "However, before we can start training, we need to convert our text into a list of integers, where each integer represents a different character. For example, \"First Citizen\" becomes:\n",
    "\n",
    "```\n",
    "Character:   F   i   r   s   t      C   i   t   i   z   e   n\n",
    "Integer:   [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dWPZjI0xHJ44",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:22:28.090112900Z",
     "start_time": "2023-09-29T10:22:28.082572200Z"
    }
   },
   "source": [
    "BATCH_SIZE = 64  # Batch size\n",
    "BUFFER_SIZE = 10000  # Buffer size to shuffle the dataset\n",
    "\n",
    "def split_input_target(chunk):\n",
    "  # Create (input_string, output_string) pairs\n",
    "  input_text = chunk[:-1]\n",
    "  target_text = chunk[1:]\n",
    "  return input_text, target_text\n",
    "\n",
    "def prepare_text(text):\n",
    "  # The unique characters in the file\n",
    "  vocab = sorted(set(text))\n",
    "  print ('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "  # Creating a mapping from unique characters to indices\n",
    "  char_map = {\n",
    "      'char_to_index': {char: index for index, char in enumerate(vocab)},\n",
    "      'index_to_char': np.array(vocab)\n",
    "  }\n",
    "\n",
    "  text_as_int = np.array([char_map['char_to_index'][c] for c in text])\n",
    "\n",
    "  # The maximum length sentence we want for a single input in characters\n",
    "  seq_length = 100\n",
    "  examples_per_epoch = len(text) // (seq_length+1)\n",
    "\n",
    "  # Create training examples / targets\n",
    "  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "  sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "  dataset = sequences.map(split_input_target)\n",
    "\n",
    "  # (TF data is designed to work with possibly infinite sequences,\n",
    "  # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "  # it maintains a buffer in which it shuffles elements).\n",
    "  dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "  return dataset, vocab, examples_per_epoch, char_map"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPGoaE2TDnX9"
   },
   "source": [
    "Now we can create and train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y5bOTe1hDqtY",
    "ExecuteTime": {
     "end_time": "2023-09-29T12:15:36.934879100Z",
     "start_time": "2023-09-29T12:15:36.933909Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Embedding(vocab_size,\n",
    "                                embedding_dim,\n",
    "                                batch_input_shape=[batch_size, None]),\n",
    "      tf.keras.layers.GRU(rnn_units,\n",
    "                          return_sequences=True,\n",
    "                          recurrent_initializer='glorot_uniform',\n",
    "                          stateful=True),\n",
    "      tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def create_model(text, epochs=3, embedding_dim=256, rnn_units=1024):\n",
    "  dataset, vocab, examples_per_epoch, char_map = prepare_text(text)\n",
    "\n",
    "  vocab_size = len(vocab)  # Length of the vocabulary in chars\n",
    "  # embedding_dim = 256  # The embedding dimension\n",
    "  # rnn_units = 1024  # Number of RNN units\n",
    "\n",
    "  model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "  # Create checkpoints once the model has been trained\n",
    "  checkpoint_dir = './training_checkpoints'\n",
    "  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_prefix,\n",
    "      save_weights_only=True)\n",
    "\n",
    "  # Train the model\n",
    "  history = model.fit(\n",
    "      dataset,\n",
    "      epochs=epochs,\n",
    "      callbacks=[checkpoint_callback])\n",
    "\n",
    "  tf.train.latest_checkpoint(checkpoint_dir)\n",
    "  model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "  model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "  model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "  return model, char_map"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tK4fcZI55rzd",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:19.317900700Z",
     "start_time": "2023-09-29T10:22:28.113945400Z"
    }
   },
   "source": [
    "shake_model, shake_chars = create_model(shakespeare)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n",
      "Epoch 1/3\n",
      "172/172 [==============================] - 214s 1s/step - loss: 2.6603\n",
      "Epoch 2/3\n",
      "172/172 [==============================] - 186s 1s/step - loss: 1.9546\n",
      "Epoch 3/3\n",
      "172/172 [==============================] - 190s 1s/step - loss: 1.6898\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_R1efC2eKH1"
   },
   "source": [
    "Now that we've trained our model, we can finally use it to generate some text. The following function takes a model and a string as input, and continually predicts and appends the next character to the string until it becomes 1,000 characters long."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KhVcs5ny-urX",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:19.324756Z",
     "start_time": "2023-09-29T10:32:19.320749600Z"
    }
   },
   "source": [
    "def generate_text(model, char_map, start_string, temperature=1.0):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  if not start_string:\n",
    "    print(\"start_string can't be empty\")\n",
    "    return \"\"\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char_map['char_to_index'][s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(char_map['index_to_char'][predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS0UlqVbhOwa"
   },
   "source": [
    "Let's generate some text!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1YOnJYAn-upC",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:25.586644900Z",
     "start_time": "2023-09-29T10:32:19.322755Z"
    }
   },
   "source": [
    "print(generate_text(shake_model, shake_chars, \"ROMEO: \", temperature=1.0))"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Indersers?\n",
      "\n",
      "MENCUTET:\n",
      "Go, sirch my bods; that se? Go the call couse, I have looks at with dier:\n",
      "Afoing Herrow!-\n",
      "LEONTES:\n",
      "Prothers; kill should so if, and risparedr nume!\n",
      "I have 'twers live,\n",
      "Arw'd lenge of yor trought thou art threazen.\n",
      "\n",
      "HESRY BOLINGBROKE:\n",
      "Hese chowery her, mut give\n",
      "With hold in.\n",
      "Dusting and mare the admigies of dares,\n",
      "Loves, or bid the nobarry pilie, har great what an well by the ne\n",
      "That him thee, Gives years, I do ass.\n",
      "\n",
      "ASTOLLO:\n",
      "Pray thou though sourte come dog.\n",
      "Or IAll this mickly parpor to enve!\n",
      "No, if a great and prince him littas thy vise!\n",
      "\n",
      "TONCESTER:\n",
      "Athis such ase med, my daughty be flemble wothim.\n",
      "\n",
      "PETRUCHIO:\n",
      "Wey, it gut shoulows from mornel!\n",
      "And resormy: but you be as? For The think or hen lime\n",
      "To a roward: thy shall Monty in theerook, which you art,\n",
      "Benest thou fless as we'll keeply parrnieve\n",
      "The devisemend. of you with you,\n",
      "stand in, morsert.\n",
      "They say that so seee, shall good deptives itseen\n",
      "With upcordleing call do as thy bose of Montious, and in the wean t\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-n5WwsuZe0B6"
   },
   "source": [
    "# Assignment\n",
    "Answer the following questions and hand in your solution in Canvas before 8:30 on Monday morning, October 2nd. Remember to save your file before uploading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoKGONy4fSl3"
   },
   "source": [
    "## Question 1\n",
    "The `temperature` parameter of `generate_text()`, defined earlier in the notebook, controls how predictable the generated text will be. The lower the temperature, the more the function will tend to append the most likely character (according to the model's prediction). A higher temperature introduces some randomness, leading to more unpredictable text.\n",
    "\n",
    "The text we generated above used a temperature of 1.0. Try generating more text using the Shakespeare model, once using a temperature of 0.2 and again using a temperature of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XsSNVxKEeGZb",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:44:54.545358100Z",
     "start_time": "2023-09-29T10:44:41.174732600Z"
    }
   },
   "source": [
    "# Your solution here\n",
    "print(\"\\n\\n-------------- TEMPERATURE 0.2 --------------\"\n",
    "      \"\\n\"+generate_text(shake_model, shake_chars, \"JULIET: \", temperature=0.2))\n",
    "print(\"\\n\\n-------------- TEMPERATURE 0.8 --------------\"\n",
    "      \"\\n\"+generate_text(shake_model, shake_chars, \"MERCUTIO: \", temperature=0.8))"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------- TEMPERATURE 0.2 --------------\n",
      "JULIET: I will be so see the seath of the seal.\n",
      "\n",
      "KING RICHARD III:\n",
      "A marred me so see the common of your such a manter.\n",
      "\n",
      "KING RICHARD II:\n",
      "And the seath the rest of the send the seems and the sears of my lord.\n",
      "\n",
      "KING RICHARD II:\n",
      "And there is the seems of the seave the marred of the see the seal the world be soul the dead of the seems of the rest of the servent of the dead and the common of the commons of the proud that shall not the dead and the seave the sees of the marreal of the seal.\n",
      "\n",
      "LEONTES:\n",
      "Go, sir, she shall be sone the seal the seal of the seath.\n",
      "\n",
      "KING RICHARD II:\n",
      "And there is the see the send the dear of the seed of the marred me to the prould the send the send the dead with the see the seather and the dead is a man and the dead of the seath of the world the sender of the see the dead the procest the see the dead of this with the seals of the first of the comes me the seather her for the sears the marred the seest the marred with the command.\n",
      "\n",
      "KING RICHARD II:\n",
      "Ay, sir, the see the comm\n",
      "\n",
      "\n",
      "-------------- TEMPERATURE 0.8 --------------\n",
      "MERCUTIO: then it see that thou sawest prial\n",
      "Mants, manter me stuth the drangs:\n",
      "I seer and my father, of the comms tool my forst busing that\n",
      "Romenger I will doing that seed is too me heart,\n",
      "And that with the will never it soliman.\n",
      "\n",
      "LUCIO:\n",
      "I'll take my mother with the words;\n",
      "The gites of GoLO:\n",
      "Hear not arm and in but the rine where's marry of the better\n",
      "To make extrought didcely give well.\n",
      "\n",
      "Swrets than see them wouldssticusan\n",
      "And mare tite of seast as so after.\n",
      "\n",
      "GLOUCESTER:\n",
      "In sunars, bear you be ruthmen.\n",
      "\n",
      "QUEENA:\n",
      "I discoster bear your soo?\n",
      "\n",
      "GLOUCESTER:\n",
      "I frierd ad a noble would then my sears are my lords in Grisked me?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "What soverengers death'd accourted,\n",
      "And think one that and I leave you.\n",
      "\n",
      "BONTOGK:\n",
      "Pase your reselfed dot seed with this marry be soms,\n",
      "And thates a grace langer on these are rese of York;\n",
      "And the that madsher childst and me here all take.\n",
      "They's wrong of the danger? What you not see is the pitions, made!\n",
      "Here is first that reserve who ars frind that me taunt.\n",
      "\n",
      "CO\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hauhMD0gfV2o"
   },
   "source": [
    "## Question 2\n",
    "NLTK's `names` corpus contains a list of approximately 8,000 English names. Train a new model on `names_raw` for at least 20 epochs using the `create_model(text, epochs=n)` function defined earlier. Use the trained model to generate a list of names (with the `generate_text` function defined earlier), starting with your own first name. Your name should not contain any non-English characters, and should end with an `\\n`.\n",
    "\n",
    "Print out the names that do not appear in the training data. Do you get any actual names (or at least names that sound plausible)?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NEOIqRE9fWTF",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:27.113550500Z",
     "start_time": "2023-09-29T10:32:25.587677600Z"
    }
   },
   "source": [
    "# Don't modify this code cell\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "nltk.download('names')\n",
    "\n",
    "# Print out a few examples\n",
    "names_raw = names.raw()\n",
    "names_unique = set(names_raw.split())\n",
    "names_raw = \"\\n\".join(names_unique)\n",
    "print(names_raw.splitlines()[:5])"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Matilda', 'Patel', 'Cayla', 'Marinna', 'Barret']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\pasqu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\names.zip.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d7Y8qE1tYR1n",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:54:58.170220Z",
     "start_time": "2023-09-29T10:51:57.493299800Z"
    }
   },
   "source": [
    "# Your solution here\n",
    "names_model, names_chars = create_model(names_raw, epochs=20)\n",
    "print(generate_text(names_model, names_chars, \"DAMIANO\\n\", temperature=1.0))\n",
    "\n",
    "# ANSWER: It appears that the model generated names similar to plausible names, but not real names."
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 unique characters\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 9s 988ms/step - loss: 4.0577\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 3.5409\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 8s 1s/step - loss: 3.1817\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 8s 1s/step - loss: 2.9812\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 8s 1s/step - loss: 2.7682\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.5632\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.4356\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 8s 1s/step - loss: 2.3622\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.3210\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2882\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2605\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2377\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2196\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2011\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1914\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1797\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1703\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1571\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1462\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1375\n",
      "DAMIANO\n",
      "Zovley\n",
      "Quin\n",
      "Heanaula\n",
      "Diratta\n",
      "Aimia\n",
      "Aleare\n",
      "Dorvara\n",
      "Hettina\n",
      "Chrissan\n",
      "Caldrinah\n",
      "Frely\n",
      "Caricorna\n",
      "Deo\n",
      "Commel\n",
      "Sil\n",
      "Joke\n",
      "Inerig\n",
      "Foli\n",
      "Emilaet\n",
      "Keejia\n",
      "Vrinita\n",
      "Roty\n",
      "Jelie\n",
      "Coronanche\n",
      "Mior\n",
      "Runy\n",
      "Chamery\n",
      "Moratt\n",
      "Orsa\n",
      "Dara\n",
      "Hineldaddi\n",
      "Jerheld\n",
      "Terili\n",
      "Derghibae\n",
      "Marny\n",
      "Reah\n",
      "Carlita\n",
      "Guencie\n",
      "Netal\n",
      "Eveline\n",
      "Juedor\n",
      "Labia\n",
      "Varite\n",
      "Kaunn\n",
      "Taquel\n",
      "Frabecton\n",
      "Radiand\n",
      "Orama\n",
      "Joan\n",
      "Mergiell\n",
      "Ehholina\n",
      "Janabrie\n",
      "Ehlin\n",
      "Kitran\n",
      "Gallol\n",
      "Marie\n",
      "Pethaus\n",
      "Shinth\n",
      "Gylyc\n",
      "Gantidrle\n",
      "Vid\n",
      "Jolia\n",
      "Gigra\n",
      "Shpid\n",
      "Lejaine\n",
      "Tigoma\n",
      "Lellen\n",
      "Oleltins\n",
      "Rachay\n",
      "Edrisse\n",
      "Genndetta\n",
      "Cardeol\n",
      "Gayddin\n",
      "Lansaw\n",
      "Shestoel\n",
      "Haullanda\n",
      "Chichichy\n",
      "Zhnete\n",
      "Ringa\n",
      "Rogh\n",
      "Edrie\n",
      "Romorde\n",
      "Yad\n",
      "Terelloe\n",
      "Marryn\n",
      "Celyn\n",
      "Koria\n",
      "Dlon\n",
      "Walmy\n",
      "Sadia\n",
      "Griedris\n",
      "Doway\n",
      "Donky\n",
      "Roza\n",
      "Merilin\n",
      "Hamberco\n",
      "Cledstia\n",
      "Orrid\n",
      "Zadrie\n",
      "Avianna\n",
      "Selande\n",
      "Marria\n",
      "Gordi\n",
      "Evwem\n",
      "Piqutto\n",
      "Anzany\n",
      "Belemz\n",
      "Doranda\n",
      "Anda\n",
      "Dado\n",
      "Elis\n",
      "Hary\n",
      "Gusi\n",
      "Fallika\n",
      "Gervie\n",
      "Hayk\n",
      "Marie\n",
      "Caika\n",
      "Ashetta\n",
      "Carri\n",
      "Tolera\n",
      "Anes\n",
      "Zeounsee\n",
      "Charmak\n",
      "Xynce\n",
      "Asmes\n",
      "Alellann\n",
      "Laille\n",
      "Emsin\n",
      "Parolet\n",
      "Geliadore\n",
      "Rommor\n",
      "Brerila\n",
      "Idnond\n",
      "Jaseet\n",
      "Hetoly\n",
      "Eshie\n",
      "Lylena\n",
      "Flatina\n",
      "Siphiel\n",
      "Kellia\n",
      "Sinus\n",
      "I\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Question 3\n",
    "The size of the model can make a difference when it comes to performance. Create a new model that has twice the number of hidden units as the previous model and double the size of the embeddings. How does the performance change? What happens if you decrease these parameters?"
   ],
   "metadata": {
    "id": "64h0OI83sXJE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your solution here\n",
    "names_model_2, names_chars_2 = create_model( names_raw, epochs=20, embedding_dim=512, rnn_units=2048)\n",
    "print(generate_text(names_model_2, names_chars_2, \"Damiano\\n\", temperature=1.0))\n",
    "names_model_3, names_chars_3 = create_model( names_raw, epochs=20, embedding_dim=64, rnn_units=256)\n",
    "print(generate_text(names_model_3, names_chars_3, \"Damiano\\n\", temperature=1.0))"
   ],
   "metadata": {
    "id": "Myk6_IUitYjk",
    "ExecuteTime": {
     "end_time": "2023-09-29T12:27:48.882942500Z",
     "start_time": "2023-09-29T12:15:46.774200700Z"
    }
   },
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 unique characters\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 8.1272\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 6.0124\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 3.6843\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 3.1752\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.8026\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 32s 4s/step - loss: 2.6072\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.5092\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.4557\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.4206\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 60s 7s/step - loss: 2.3874\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.3603\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.3337\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.3061\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 34s 4s/step - loss: 2.2863\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 34s 4s/step - loss: 2.2716\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 34s 4s/step - loss: 2.2606\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 37s 5s/step - loss: 2.2462\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 36s 5s/step - loss: 2.2343\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 34s 4s/step - loss: 2.2243\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 34s 4s/step - loss: 2.2139\n",
      "Damiano\n",
      "Cahesqueeaie-auamamaayyneeeaeaiatanaapeata-Eierearayabalieaazeaatanaaynanaleranatana\n",
      "Nantananalatataaarenatana\n",
      "Miaia\n",
      "Baniaa\n",
      "Agkaneangareaanataa\n",
      "Adaroaelaraha\n",
      "Esslyayneatardaiananadeyata\n",
      "Latta-Caranffhadaneetenaarauttatalaraamaanadaialaba\n",
      "Haudfana-Watadatatanaeeadada\n",
      "Nestajayaneethautt\n",
      "Dafapia\n",
      "Osahefffaynasatty\n",
      "Camarienkaataasaeaahafatte\n",
      "Tistaralahaylarealana\n",
      "Wiakanagana\n",
      "Estonanarananaroneetanasanattasa\n",
      "Tingeana\n",
      "Kamfaina\n",
      "Whathahana\n",
      "Batadinananarauenemy\n",
      "Naendara\n",
      "Adarayakda\n",
      "Savanabcealliak\n",
      "Liadanaaaeeiceneta\n",
      "Maydaratanamadanaphaaia\n",
      "Evanara\n",
      "Ramafetada\n",
      "Mariellia-Leraana\n",
      "Buisa\n",
      "Heardasqueladrameaop\n",
      "Caanarana\n",
      "Ignelaa\n",
      "Tatana-Janadfarstara\n",
      "Banedaaedaia\n",
      "DaMaugunasataa\n",
      "Damandatteataa\n",
      "Qudawa\n",
      "Saveg\n",
      "Caradialgaiatacsadae\n",
      "Mauga\n",
      "Cathathaeana\n",
      "Emaay\n",
      "Dameguenelea\n",
      "Wasthanani\n",
      "Ebalaba\n",
      "Granarda\n",
      "Nieenata\n",
      "Marthe\n",
      "Garkeiatala\n",
      "Finanana\n",
      "Hartanaaydayearaazaa\n",
      "Brantellaanena\n",
      "Handadey\n",
      "Maanararra\n",
      "Laatadane\n",
      "Latalama\n",
      "Jicyan-Aranayamatananatcia\n",
      "Kyaita\n",
      "Waramayna\n",
      "Wan-Candandandyarasa\n",
      "Carra\n",
      "JalKataya\n",
      "Vackea\n",
      "Arrayna\n",
      "Liagharenda\n",
      "55 unique characters\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 3.9356\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 3.5253\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 1s 169ms/step - loss: 3.3294\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 3.2090\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 3.1413\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 3.1092\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 3.0564\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 2.9853\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 2.8846\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 2.7579\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 1s 167ms/step - loss: 2.6305\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 1s 167ms/step - loss: 2.5321\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 1s 167ms/step - loss: 2.4622\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 2.4170\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 2.3888\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 2.3663\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 1s 162ms/step - loss: 2.3488\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 2.3322\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 2.3160\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 2.3023\n",
      "Damiano\n",
      "Jrabrin\n",
      "Covdy\n",
      "Masice\n",
      "Alahyl\n",
      "Teli\n",
      "Rorronne\n",
      "Lalreulhta\n",
      "Lesole\n",
      "Jinjanodl\n",
      "Erod\n",
      "Zallil\n",
      "Barrena\n",
      "Kelvan\n",
      "Prate\n",
      "Kmeta\n",
      "Derhor\n",
      "Horiatma\n",
      "Karthie\n",
      "Pollis\n",
      "Keryt\n",
      "Dafela\n",
      "Ginnogece\n",
      "Darlie\n",
      "Morlil\n",
      "Lana\n",
      "Mardytce\n",
      "Rl\n",
      "Buta\n",
      "Ginala\n",
      "Marko\n",
      "Neial\n",
      "Ildrayn\n",
      "Vital\n",
      "Corhina\n",
      "Jubgini\n",
      "Tarcs\n",
      "Tuatla\n",
      "Ruhve\n",
      "Armya\n",
      "Anl\n",
      "Kicel\n",
      "Ameli\n",
      "Culheckis\n",
      "Gispen\n",
      "Pharba\n",
      "Wedelia\n",
      "Halllin\n",
      "Kaigisedta\n",
      "Beitane\n",
      "Manene\n",
      "Euan\n",
      "Oposha\n",
      "Emtey\n",
      "Logra\n",
      "Cundy\n",
      "Belnan\n",
      "Jorrin\n",
      "Robolhla\n",
      "Deesa\n",
      "Nerisa\n",
      "Jaaves\n",
      "RogqUqUlomre\n",
      "Bysole\n",
      "Jusinl\n",
      "She\n",
      "Garetha\n",
      "Zatomoc\n",
      "Thincoa\n",
      "Arrtha\n",
      "Srmard\n",
      "Jonye\n",
      "Karlart\n",
      "Fule\n",
      "Damercane\n",
      "Heryn\n",
      "Atleshay\n",
      "Alarsede\n",
      "Macehsa\n",
      "Donnen\n",
      "Dants\n",
      "Koroiti\n",
      "Bunn\n",
      "Rargeltie\n",
      "Zeazi\n",
      "Rotie\n",
      "Erclan\n",
      "Eler\n",
      "Mance\n",
      "Bagell\n",
      "Amonnald\n",
      "Qodlra\n",
      "Daehiet\n",
      "Elon\n",
      "Ranlo\n",
      "Handara\n",
      "Ertetse\n",
      "AnnardZaa\n",
      "Fablilesa\n",
      "Rania\n",
      "Malmet\n",
      "Kely\n",
      "Leli\n",
      "Almiy\n",
      "Corit\n",
      "Kambendta\n",
      "Marera\n",
      "ParAn\n",
      "Redslli\n",
      "Lerneina\n",
      "Forinhik\n",
      "Gonille\n",
      "Selcorh\n",
      "Sitle\n",
      "Meysna\n",
      "Lilili\n",
      "Furyo\n",
      "Lo\n",
      "Perure\n",
      "Sorra\n",
      "Radce\n",
      "Lore\n",
      "Deadi\n",
      "Corgra\n",
      "Alda\n",
      "Dellien\n",
      "Aninis\n",
      "Mapel\n",
      "Heitha\n",
      "Nihhent\n",
      "Lierin\n",
      "Huis\n",
      "Wesy\n",
      "Heue\n",
      "Ceivine\n",
      "Mamino\n",
      "Zeticiephi\n",
      "Malise\n",
      "Vomuya\n",
      "Gadell\n",
      "Ka\n",
      "Mella\n",
      "Silbais\n",
      "Lwa\n",
      "G\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 4\n",
    "Transformer large language models can also generate text. The following code imports a pretrained GPT-2 model from Huggingface's Transformer library. This model can then be used directly to generate text, given a prompt as context. Alter the prompt to have the transformer model (GPT-2) generate an engaging story beginning using one of the following story starters:\n",
    "\n",
    "\n",
    "*   It was the day the moon fell.\n",
    "*   Am I in heaven?  What happened to me?\n",
    "*   Wandering through the graveyard it felt like something was watching me.\n",
    "*   Three of us.  We were the only ones left, the only ones to make it to the island.\n",
    "\n",
    "There are several different methods to choose from to generate the text (as seen in the commented out lines below). Try out the different methods and play with the parameters. This [blogpost](https://huggingface.co/blog/how-to-generate) explains their differences.\n",
    "\n",
    "Which method has the best performance?\n",
    "\n",
    "Can GPT-2 generate Shakespere?"
   ],
   "metadata": {
    "id": "2FI-1ldhn0SE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Uncomment if transformers is not installed\n",
    "!pip install transformers"
   ],
   "metadata": {
    "id": "GE-xTvvYvHAy",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:29.438474Z",
     "start_time": "2023-09-29T10:32:27.117583200Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.33.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pasqu\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Do not modify this code\n",
    "# https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Today I believe we can finally\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=100) # Greedy search\n",
    "#outputs = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=3, early_stopping=True) # Beam search\n",
    "#outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=0, temperature=0.7) # Sampling\n",
    "#outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=50) # Top-k\n",
    "#outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=50, top_p=0.92) # Top-p\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ],
   "metadata": {
    "id": "2_ZMgywnnziH",
    "ExecuteTime": {
     "end_time": "2023-09-29T12:39:17.108319800Z",
     "start_time": "2023-09-29T12:37:46.716718400Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c90754c63f94f6099a83bbde8110277"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pasqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pasqu\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (â€¦)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de92a731766344d3bee850b87d15b47b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can make a difference in the lives of the people of the United States of America.\\n\\n']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Your solution here\n",
    "\n",
    "prompts = [\n",
    "  \"It was the day the moon fell.\",\n",
    "  \"Am I in heaven?  What happened to me?\",\n",
    "  \"Wandering through the graveyard it felt like something was watching me.\",\n",
    "  \"Three of us.  We were the only ones left, the only ones to make it to the island.\"\n",
    "]\n",
    "\n",
    "def run_gpt2(prompt, method=\"greedy\"):\n",
    "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "  if method == \"greedy\":\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "  elif method == \"beam\":\n",
    "    outputs = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=3, early_stopping=True)\n",
    "  elif method == \"sampling\":\n",
    "    outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=0, temperature=0.7)\n",
    "  elif method == \"topk\":\n",
    "    outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=50)\n",
    "  elif method == \"topp\":\n",
    "    outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=50, top_p=0.92)\n",
    "  else:\n",
    "    print(\"Invalid method\")\n",
    "    return\n",
    "  print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "print(\"\\n\\n-------------- Greedy search --------------\")\n",
    "run_gpt2(prompts[3], method=\"greedy\")\n",
    "print(\"\\n\\n-------------- Beam search --------------\")\n",
    "run_gpt2(prompts[3], method=\"beam\")\n",
    "print(\"\\n\\n-------------- Sampling --------------\")\n",
    "run_gpt2(prompts[3], method=\"sampling\")\n",
    "print(\"\\n\\n-------------- Top-k --------------\")\n",
    "run_gpt2(prompts[3], method=\"topk\")\n",
    "print(\"\\n\\n-------------- Top-p --------------\")\n",
    "run_gpt2(prompts[3], method=\"topp\")\n",
    "\n",
    "# ANSWER: The top-k method seems to have the best performance, but also the sampling had good performance. Shakespeare is not generated."
   ],
   "metadata": {
    "id": "hnH32YYKraN4",
    "ExecuteTime": {
     "end_time": "2023-09-29T13:36:54.285209300Z",
     "start_time": "2023-09-29T13:36:37.453704800Z"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------- Greedy search --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Three of us.  We were the only ones left, the only ones to make it to the island.  We were the only ones to make it to the island.  We were the only ones to make it to the island.  We were the only ones to make it to the island.  We were the only ones to make it to the island.  We were the only ones to make it to the island.  We were the only ones to make it to the island.']\n",
      "\n",
      "\n",
      "-------------- Beam search --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Three of us.  We were the only ones left, the only ones to make it to the island.  It was the only place we could go.  And we were the ones who made it.  The only ones who could make it.\\n\\n\"I\\'m sorry.  I didn\\'t mean to hurt you.  But I don\\'t know how you feel about me.  You don\\'t deserve to be here.  That\\'s why you\\'re here.\"\\n\\nI was']\n",
      "\n",
      "\n",
      "-------------- Sampling --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Three of us.  We were the only ones left, the only ones to make it to the island.  We were not going to let those small villages die. We were not going to give them to anyone. We were going to take things we knew could not be taken.  That's what we did. That's what we do: We take things, and we go with them.  That's what we do: We move forward, and we start over.  That's what\"]\n",
      "\n",
      "\n",
      "-------------- Top-k --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Three of us.  We were the only ones left, the only ones to make it to the island.  It seems like our days without any of them came later and we'll always remember this part of the book.  I mean our lives, not only because of them, but also to have some of the stories that came afterwards, which had a sense of the story that was not lost.  It's so amazing to me to read with such an effect and see the effect so very\"]\n",
      "\n",
      "\n",
      "-------------- Top-p --------------\n",
      "[\"Three of us.  We were the only ones left, the only ones to make it to the island.  We made a mistake, and it was too late. And now it is too late again.  \\xa0But our people know what I will tell them.  We will call it the Baha'i Faith. We will say that Jesus has made it possible for you to see in all the lives of your children a God who is a true, loving, and true God.\"]\n"
     ]
    }
   ]
  }
 ]
}
