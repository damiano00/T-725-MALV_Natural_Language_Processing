{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3IOeFoye2P8"
   },
   "source": [
    "# T-725 Natural Language Processing: Lab 5\n",
    "In today's lab, we will be working with neural networks, using GRUs and Transformers for text generation.\n",
    "\n",
    "To begin with, do the following:\n",
    "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
    "* **Select `\"Runtime\" > \"Change runtime type\"`, and make sure that you have \"Hardware accelerator\" set to \"GPU\"**\n",
    "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R7ElxTOtl6UQ",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:22:24.435845300Z",
     "start_time": "2023-09-29T10:22:24.333079400Z"
    }
   },
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress some warnings from TensorFlow about deprecated functions\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ayz3HiU7JvCF"
   },
   "source": [
    "## Generating text with neural networks\n",
    "Let's create a neural language model and use it to generate some text. This time, we will use character embeddings rather than word embeddings. They are created in exactly the same way, and are often used together in neural network-based models. One benefit of using character embeddings is that we can generate words that our model has never seen before.\n",
    "\n",
    "The model takes as input a sequence of characters and predicts which character is most likely to follow. We will generate text by repeatedly predicting and appending the next character to a string. First, however, we need some text to train it on.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PN7I_djD91Py",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:22:28.077569400Z",
     "start_time": "2023-09-29T10:22:24.342052700Z"
    }
   },
   "source": [
    "# Based on the following tutorial:\n",
    "# https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Let's download some text by Shakespeare to train our model\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)\n",
    "\n",
    "with open(path_to_file, encoding='utf-8') as f:\n",
    "  shakespeare = f.read()\n",
    "\n",
    "print(\"First 250 characters:\")\n",
    "print(shakespeare[:250])\n",
    "\n",
    "print (\"Length of text: {:,} characters\".format(len(shakespeare)))"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n",
      "First 250 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "Length of text: 1,115,394 characters\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45JqVDxqtf1o"
   },
   "source": [
    "Now we can create training examples for our model. Each example will be a pair of strings: one input string containing 100 characters, and a target string that is one character ahead. For example, the first pair we create is:\n",
    "\n",
    "**Input string**:  `'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'`\n",
    "\n",
    "**Target string**: `'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '`\n",
    "\n",
    "However, before we can start training, we need to convert our text into a list of integers, where each integer represents a different character. For example, \"First Citizen\" becomes:\n",
    "\n",
    "```\n",
    "Character:   F   i   r   s   t      C   i   t   i   z   e   n\n",
    "Integer:   [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dWPZjI0xHJ44",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:22:28.090112900Z",
     "start_time": "2023-09-29T10:22:28.082572200Z"
    }
   },
   "source": [
    "BATCH_SIZE = 64  # Batch size\n",
    "BUFFER_SIZE = 10000  # Buffer size to shuffle the dataset\n",
    "\n",
    "def split_input_target(chunk):\n",
    "  # Create (input_string, output_string) pairs\n",
    "  input_text = chunk[:-1]\n",
    "  target_text = chunk[1:]\n",
    "  return input_text, target_text\n",
    "\n",
    "def prepare_text(text):\n",
    "  # The unique characters in the file\n",
    "  vocab = sorted(set(text))\n",
    "  print ('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "  # Creating a mapping from unique characters to indices\n",
    "  char_map = {\n",
    "      'char_to_index': {char: index for index, char in enumerate(vocab)},\n",
    "      'index_to_char': np.array(vocab)\n",
    "  }\n",
    "\n",
    "  text_as_int = np.array([char_map['char_to_index'][c] for c in text])\n",
    "\n",
    "  # The maximum length sentence we want for a single input in characters\n",
    "  seq_length = 100\n",
    "  examples_per_epoch = len(text) // (seq_length+1)\n",
    "\n",
    "  # Create training examples / targets\n",
    "  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "  sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "  dataset = sequences.map(split_input_target)\n",
    "\n",
    "  # (TF data is designed to work with possibly infinite sequences,\n",
    "  # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "  # it maintains a buffer in which it shuffles elements).\n",
    "  dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "  return dataset, vocab, examples_per_epoch, char_map"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPGoaE2TDnX9"
   },
   "source": [
    "Now we can create and train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y5bOTe1hDqtY",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:22:28.097383300Z",
     "start_time": "2023-09-29T10:22:28.094113200Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Embedding(vocab_size,\n",
    "                                embedding_dim,\n",
    "                                batch_input_shape=[batch_size, None]),\n",
    "      tf.keras.layers.GRU(rnn_units,\n",
    "                          return_sequences=True,\n",
    "                          recurrent_initializer='glorot_uniform',\n",
    "                          stateful=True),\n",
    "      tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def create_model(text, epochs=3):\n",
    "  dataset, vocab, examples_per_epoch, char_map = prepare_text(text)\n",
    "\n",
    "  vocab_size = len(vocab)  # Length of the vocabulary in chars\n",
    "  embedding_dim = 256  # The embedding dimension\n",
    "  rnn_units = 1024  # Number of RNN units\n",
    "\n",
    "  model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "  # Create checkpoints once the model has been trained\n",
    "  checkpoint_dir = './training_checkpoints'\n",
    "  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_prefix,\n",
    "      save_weights_only=True)\n",
    "\n",
    "  # Train the model\n",
    "  history = model.fit(\n",
    "      dataset,\n",
    "      epochs=epochs,\n",
    "      callbacks=[checkpoint_callback])\n",
    "\n",
    "  tf.train.latest_checkpoint(checkpoint_dir)\n",
    "  model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "  model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "  model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "  return model, char_map"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tK4fcZI55rzd",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:19.317900700Z",
     "start_time": "2023-09-29T10:22:28.113945400Z"
    }
   },
   "source": [
    "shake_model, shake_chars = create_model(shakespeare)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n",
      "Epoch 1/3\n",
      "172/172 [==============================] - 214s 1s/step - loss: 2.6603\n",
      "Epoch 2/3\n",
      "172/172 [==============================] - 186s 1s/step - loss: 1.9546\n",
      "Epoch 3/3\n",
      "172/172 [==============================] - 190s 1s/step - loss: 1.6898\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_R1efC2eKH1"
   },
   "source": [
    "Now that we've trained our model, we can finally use it to generate some text. The following function takes a model and a string as input, and continually predicts and appends the next character to the string until it becomes 1,000 characters long."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KhVcs5ny-urX",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:19.324756Z",
     "start_time": "2023-09-29T10:32:19.320749600Z"
    }
   },
   "source": [
    "def generate_text(model, char_map, start_string, temperature=1.0):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  if not start_string:\n",
    "    print(\"start_string can't be empty\")\n",
    "    return \"\"\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char_map['char_to_index'][s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(char_map['index_to_char'][predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS0UlqVbhOwa"
   },
   "source": [
    "Let's generate some text!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1YOnJYAn-upC",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:25.586644900Z",
     "start_time": "2023-09-29T10:32:19.322755Z"
    }
   },
   "source": [
    "print(generate_text(shake_model, shake_chars, \"ROMEO: \", temperature=1.0))"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Indersers?\n",
      "\n",
      "MENCUTET:\n",
      "Go, sirch my bods; that se? Go the call couse, I have looks at with dier:\n",
      "Afoing Herrow!-\n",
      "LEONTES:\n",
      "Prothers; kill should so if, and risparedr nume!\n",
      "I have 'twers live,\n",
      "Arw'd lenge of yor trought thou art threazen.\n",
      "\n",
      "HESRY BOLINGBROKE:\n",
      "Hese chowery her, mut give\n",
      "With hold in.\n",
      "Dusting and mare the admigies of dares,\n",
      "Loves, or bid the nobarry pilie, har great what an well by the ne\n",
      "That him thee, Gives years, I do ass.\n",
      "\n",
      "ASTOLLO:\n",
      "Pray thou though sourte come dog.\n",
      "Or IAll this mickly parpor to enve!\n",
      "No, if a great and prince him littas thy vise!\n",
      "\n",
      "TONCESTER:\n",
      "Athis such ase med, my daughty be flemble wothim.\n",
      "\n",
      "PETRUCHIO:\n",
      "Wey, it gut shoulows from mornel!\n",
      "And resormy: but you be as? For The think or hen lime\n",
      "To a roward: thy shall Monty in theerook, which you art,\n",
      "Benest thou fless as we'll keeply parrnieve\n",
      "The devisemend. of you with you,\n",
      "stand in, morsert.\n",
      "They say that so seee, shall good deptives itseen\n",
      "With upcordleing call do as thy bose of Montious, and in the wean t\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-n5WwsuZe0B6"
   },
   "source": [
    "# Assignment\n",
    "Answer the following questions and hand in your solution in Canvas before 8:30 on Monday morning, October 2nd. Remember to save your file before uploading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoKGONy4fSl3"
   },
   "source": [
    "## Question 1\n",
    "The `temperature` parameter of `generate_text()`, defined earlier in the notebook, controls how predictable the generated text will be. The lower the temperature, the more the function will tend to append the most likely character (according to the model's prediction). A higher temperature introduces some randomness, leading to more unpredictable text.\n",
    "\n",
    "The text we generated above used a temperature of 1.0. Try generating more text using the Shakespeare model, once using a temperature of 0.2 and again using a temperature of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XsSNVxKEeGZb",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:44:54.545358100Z",
     "start_time": "2023-09-29T10:44:41.174732600Z"
    }
   },
   "source": [
    "# Your solution here\n",
    "print(\"\\n\\n-------------- TEMPERATURE 0.2 --------------\"\n",
    "      \"\\n\"+generate_text(shake_model, shake_chars, \"JULIET: \", temperature=0.2))\n",
    "print(\"\\n\\n-------------- TEMPERATURE 0.8 --------------\"\n",
    "      \"\\n\"+generate_text(shake_model, shake_chars, \"MERCUTIO: \", temperature=0.8))"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------- TEMPERATURE 0.2 --------------\n",
      "JULIET: I will be so see the seath of the seal.\n",
      "\n",
      "KING RICHARD III:\n",
      "A marred me so see the common of your such a manter.\n",
      "\n",
      "KING RICHARD II:\n",
      "And the seath the rest of the send the seems and the sears of my lord.\n",
      "\n",
      "KING RICHARD II:\n",
      "And there is the seems of the seave the marred of the see the seal the world be soul the dead of the seems of the rest of the servent of the dead and the common of the commons of the proud that shall not the dead and the seave the sees of the marreal of the seal.\n",
      "\n",
      "LEONTES:\n",
      "Go, sir, she shall be sone the seal the seal of the seath.\n",
      "\n",
      "KING RICHARD II:\n",
      "And there is the see the send the dear of the seed of the marred me to the prould the send the send the dead with the see the seather and the dead is a man and the dead of the seath of the world the sender of the see the dead the procest the see the dead of this with the seals of the first of the comes me the seather her for the sears the marred the seest the marred with the command.\n",
      "\n",
      "KING RICHARD II:\n",
      "Ay, sir, the see the comm\n",
      "\n",
      "\n",
      "-------------- TEMPERATURE 0.8 --------------\n",
      "MERCUTIO: then it see that thou sawest prial\n",
      "Mants, manter me stuth the drangs:\n",
      "I seer and my father, of the comms tool my forst busing that\n",
      "Romenger I will doing that seed is too me heart,\n",
      "And that with the will never it soliman.\n",
      "\n",
      "LUCIO:\n",
      "I'll take my mother with the words;\n",
      "The gites of GoLO:\n",
      "Hear not arm and in but the rine where's marry of the better\n",
      "To make extrought didcely give well.\n",
      "\n",
      "Swrets than see them wouldssticusan\n",
      "And mare tite of seast as so after.\n",
      "\n",
      "GLOUCESTER:\n",
      "In sunars, bear you be ruthmen.\n",
      "\n",
      "QUEENA:\n",
      "I discoster bear your soo?\n",
      "\n",
      "GLOUCESTER:\n",
      "I frierd ad a noble would then my sears are my lords in Grisked me?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "What soverengers death'd accourted,\n",
      "And think one that and I leave you.\n",
      "\n",
      "BONTOGK:\n",
      "Pase your reselfed dot seed with this marry be soms,\n",
      "And thates a grace langer on these are rese of York;\n",
      "And the that madsher childst and me here all take.\n",
      "They's wrong of the danger? What you not see is the pitions, made!\n",
      "Here is first that reserve who ars frind that me taunt.\n",
      "\n",
      "CO\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hauhMD0gfV2o"
   },
   "source": [
    "## Question 2\n",
    "NLTK's `names` corpus contains a list of approximately 8,000 English names. Train a new model on `names_raw` for at least 20 epochs using the `create_model(text, epochs=n)` function defined earlier. Use the trained model to generate a list of names (with the `generate_text` function defined earlier), starting with your own first name. Your name should not contain any non-English characters, and should end with an `\\n`.\n",
    "\n",
    "Print out the names that do not appear in the training data. Do you get any actual names (or at least names that sound plausible)?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NEOIqRE9fWTF",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:27.113550500Z",
     "start_time": "2023-09-29T10:32:25.587677600Z"
    }
   },
   "source": [
    "# Don't modify this code cell\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "nltk.download('names')\n",
    "\n",
    "# Print out a few examples\n",
    "names_raw = names.raw()\n",
    "names_unique = set(names_raw.split())\n",
    "names_raw = \"\\n\".join(names_unique)\n",
    "print(names_raw.splitlines()[:5])"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Matilda', 'Patel', 'Cayla', 'Marinna', 'Barret']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\pasqu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\names.zip.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d7Y8qE1tYR1n",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:54:58.170220Z",
     "start_time": "2023-09-29T10:51:57.493299800Z"
    }
   },
   "source": [
    "# Your solution here\n",
    "names_model, names_chars = create_model(names_raw, epochs=20)\n",
    "print(generate_text(names_model, names_chars, \"DAMIANO\\n\", temperature=1.0))"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 unique characters\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 9s 988ms/step - loss: 4.0577\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 3.5409\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 8s 1s/step - loss: 3.1817\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 8s 1s/step - loss: 2.9812\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 8s 1s/step - loss: 2.7682\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.5632\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.4356\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 8s 1s/step - loss: 2.3622\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.3210\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2882\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2605\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2377\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2196\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2011\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1914\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1797\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1703\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1571\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1462\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1375\n",
      "DAMIANO\n",
      "Zovley\n",
      "Quin\n",
      "Heanaula\n",
      "Diratta\n",
      "Aimia\n",
      "Aleare\n",
      "Dorvara\n",
      "Hettina\n",
      "Chrissan\n",
      "Caldrinah\n",
      "Frely\n",
      "Caricorna\n",
      "Deo\n",
      "Commel\n",
      "Sil\n",
      "Joke\n",
      "Inerig\n",
      "Foli\n",
      "Emilaet\n",
      "Keejia\n",
      "Vrinita\n",
      "Roty\n",
      "Jelie\n",
      "Coronanche\n",
      "Mior\n",
      "Runy\n",
      "Chamery\n",
      "Moratt\n",
      "Orsa\n",
      "Dara\n",
      "Hineldaddi\n",
      "Jerheld\n",
      "Terili\n",
      "Derghibae\n",
      "Marny\n",
      "Reah\n",
      "Carlita\n",
      "Guencie\n",
      "Netal\n",
      "Eveline\n",
      "Juedor\n",
      "Labia\n",
      "Varite\n",
      "Kaunn\n",
      "Taquel\n",
      "Frabecton\n",
      "Radiand\n",
      "Orama\n",
      "Joan\n",
      "Mergiell\n",
      "Ehholina\n",
      "Janabrie\n",
      "Ehlin\n",
      "Kitran\n",
      "Gallol\n",
      "Marie\n",
      "Pethaus\n",
      "Shinth\n",
      "Gylyc\n",
      "Gantidrle\n",
      "Vid\n",
      "Jolia\n",
      "Gigra\n",
      "Shpid\n",
      "Lejaine\n",
      "Tigoma\n",
      "Lellen\n",
      "Oleltins\n",
      "Rachay\n",
      "Edrisse\n",
      "Genndetta\n",
      "Cardeol\n",
      "Gayddin\n",
      "Lansaw\n",
      "Shestoel\n",
      "Haullanda\n",
      "Chichichy\n",
      "Zhnete\n",
      "Ringa\n",
      "Rogh\n",
      "Edrie\n",
      "Romorde\n",
      "Yad\n",
      "Terelloe\n",
      "Marryn\n",
      "Celyn\n",
      "Koria\n",
      "Dlon\n",
      "Walmy\n",
      "Sadia\n",
      "Griedris\n",
      "Doway\n",
      "Donky\n",
      "Roza\n",
      "Merilin\n",
      "Hamberco\n",
      "Cledstia\n",
      "Orrid\n",
      "Zadrie\n",
      "Avianna\n",
      "Selande\n",
      "Marria\n",
      "Gordi\n",
      "Evwem\n",
      "Piqutto\n",
      "Anzany\n",
      "Belemz\n",
      "Doranda\n",
      "Anda\n",
      "Dado\n",
      "Elis\n",
      "Hary\n",
      "Gusi\n",
      "Fallika\n",
      "Gervie\n",
      "Hayk\n",
      "Marie\n",
      "Caika\n",
      "Ashetta\n",
      "Carri\n",
      "Tolera\n",
      "Anes\n",
      "Zeounsee\n",
      "Charmak\n",
      "Xynce\n",
      "Asmes\n",
      "Alellann\n",
      "Laille\n",
      "Emsin\n",
      "Parolet\n",
      "Geliadore\n",
      "Rommor\n",
      "Brerila\n",
      "Idnond\n",
      "Jaseet\n",
      "Hetoly\n",
      "Eshie\n",
      "Lylena\n",
      "Flatina\n",
      "Siphiel\n",
      "Kellia\n",
      "Sinus\n",
      "I\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Question 3\n",
    "The size of the model can make a difference when it comes to performance. Create a new model that has twice the number of hidden units as the previous model and double the size of the embeddings. How does the performance change? What happens if you decrease these parameters?"
   ],
   "metadata": {
    "id": "64h0OI83sXJE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your solution here\n"
   ],
   "metadata": {
    "id": "Myk6_IUitYjk",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:27.119875Z",
     "start_time": "2023-09-29T10:32:27.113550500Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 4\n",
    "Transformer large language models can also generate text. The following code imports a pretrained GPT-2 model from Huggingface's Transformer library. This model can then be used directly to generate text, given a prompt as context. Alter the prompt to have the transformer model (GPT-2) generate an engaging story beginning using one of the following story starters:\n",
    "\n",
    "\n",
    "*   It was the day the moon fell.\n",
    "*   Am I in heaven?  What happened to me?\n",
    "*   Wandering through the graveyard it felt like something was watching me.\n",
    "*   Three of us.  We were the only ones left, the only ones to make it to the island.\n",
    "\n",
    "There are several different methods to choose from to generate the text (as seen in the commented out lines below). Try out the different methods and play with the parameters. This [blogpost](https://huggingface.co/blog/how-to-generate) explains their differences.\n",
    "\n",
    "Which method has the best performance?\n",
    "\n",
    "Can GPT-2 generate Shakespere?"
   ],
   "metadata": {
    "id": "2FI-1ldhn0SE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Uncomment if transformers is not installed\n",
    "!pip install transformers"
   ],
   "metadata": {
    "id": "GE-xTvvYvHAy",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:29.438474Z",
     "start_time": "2023-09-29T10:32:27.117583200Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.33.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pasqu\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pasqu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Do not modify this code\n",
    "# https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Today I believe we can finally\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=100) # Greedy search\n",
    "#outputs = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=3, early_stopping=True) # Beam search\n",
    "#outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=0, temperature=0.7) # Sampling\n",
    "#outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=50) # Top-k\n",
    "#outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=50, top_p=0.92) # Top-p\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ],
   "metadata": {
    "id": "2_ZMgywnnziH",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:33.363325800Z",
     "start_time": "2023-09-29T10:32:29.437507300Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc42c67a491a42539c8efe7747e6b6cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pasqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pasqu\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1c77cc5a9b14bd392b7343f066ff4b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40bdea5cdd71420d8e01f1ec95b4f4e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6b4fd1237ac4fd29628da5821d5025e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForCausalLM\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 8\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001B[0;32m      6\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     10\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToday I believe we can finally\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     12\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m tokenizer(prompt, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39minput_ids\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1124\u001B[0m, in \u001B[0;36mDummyObject.__getattribute__\u001B[1;34m(cls, key)\u001B[0m\n\u001B[0;32m   1122\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m key \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_from_config\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattribute__\u001B[39m(key)\n\u001B[1;32m-> 1124\u001B[0m \u001B[43mrequires_backends\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backends\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1103\u001B[0m, in \u001B[0;36mrequires_backends\u001B[1;34m(obj, backends)\u001B[0m\n\u001B[0;32m   1101\u001B[0m \u001B[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001B[39;00m\n\u001B[0;32m   1102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_available() \u001B[38;5;129;01mand\u001B[39;00m is_tf_available():\n\u001B[1;32m-> 1103\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001B[38;5;241m.\u001B[39mformat(name))\n\u001B[0;32m   1105\u001B[0m \u001B[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001B[39;00m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m is_torch_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tf_available():\n",
      "\u001B[1;31mImportError\u001B[0m: \nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForCausalLM\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Your solution here\n"
   ],
   "metadata": {
    "id": "hnH32YYKraN4",
    "ExecuteTime": {
     "start_time": "2023-09-29T10:32:33.360325700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
