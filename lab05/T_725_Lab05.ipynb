{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3IOeFoye2P8"
   },
   "source": [
    "# T-725 Natural Language Processing: Lab 5\n",
    "In today's lab, we will be working with neural networks, using GRUs and Transformers for text generation.\n",
    "\n",
    "To begin with, do the following:\n",
    "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
    "* **Select `\"Runtime\" > \"Change runtime type\"`, and make sure that you have \"Hardware accelerator\" set to \"GPU\"**\n",
    "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R7ElxTOtl6UQ",
    "ExecuteTime": {
     "end_time": "2023-09-29T14:43:35.191744400Z",
     "start_time": "2023-09-29T14:43:35.186980900Z"
    }
   },
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress some warnings from TensorFlow about deprecated functions\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ayz3HiU7JvCF"
   },
   "source": [
    "## Generating text with neural networks\n",
    "Let's create a neural language model and use it to generate some text. This time, we will use character embeddings rather than word embeddings. They are created in exactly the same way, and are often used together in neural network-based models. One benefit of using character embeddings is that we can generate words that our model has never seen before.\n",
    "\n",
    "The model takes as input a sequence of characters and predicts which character is most likely to follow. We will generate text by repeatedly predicting and appending the next character to a string. First, however, we need some text to train it on.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PN7I_djD91Py",
    "ExecuteTime": {
     "end_time": "2023-09-30T00:34:32.385689300Z",
     "start_time": "2023-09-30T00:34:32.275331100Z"
    }
   },
   "source": [
    "# Based on the following tutorial:\n",
    "# https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Let's download some text by Shakespeare to train our model\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)\n",
    "\n",
    "with open(path_to_file, encoding='utf-8') as f:\n",
    "  shakespeare = f.read()\n",
    "\n",
    "print(\"First 250 characters:\")\n",
    "print(shakespeare[:250])\n",
    "\n",
    "print (\"Length of text: {:,} characters\".format(len(shakespeare)))"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 250 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "Length of text: 1,115,394 characters\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45JqVDxqtf1o"
   },
   "source": [
    "Now we can create training examples for our model. Each example will be a pair of strings: one input string containing 100 characters, and a target string that is one character ahead. For example, the first pair we create is:\n",
    "\n",
    "**Input string**:  `'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'`\n",
    "\n",
    "**Target string**: `'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '`\n",
    "\n",
    "However, before we can start training, we need to convert our text into a list of integers, where each integer represents a different character. For example, \"First Citizen\" becomes:\n",
    "\n",
    "```\n",
    "Character:   F   i   r   s   t      C   i   t   i   z   e   n\n",
    "Integer:   [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dWPZjI0xHJ44",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:22:28.090112900Z",
     "start_time": "2023-09-29T10:22:28.082572200Z"
    }
   },
   "source": [
    "BATCH_SIZE = 64  # Batch size\n",
    "BUFFER_SIZE = 10000  # Buffer size to shuffle the dataset\n",
    "\n",
    "def split_input_target(chunk):\n",
    "  # Create (input_string, output_string) pairs\n",
    "  input_text = chunk[:-1]\n",
    "  target_text = chunk[1:]\n",
    "  return input_text, target_text\n",
    "\n",
    "def prepare_text(text):\n",
    "  # The unique characters in the file\n",
    "  vocab = sorted(set(text))\n",
    "  print ('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "  # Creating a mapping from unique characters to indices\n",
    "  char_map = {\n",
    "      'char_to_index': {char: index for index, char in enumerate(vocab)},\n",
    "      'index_to_char': np.array(vocab)\n",
    "  }\n",
    "\n",
    "  text_as_int = np.array([char_map['char_to_index'][c] for c in text])\n",
    "\n",
    "  # The maximum length sentence we want for a single input in characters\n",
    "  seq_length = 100\n",
    "  examples_per_epoch = len(text) // (seq_length+1)\n",
    "\n",
    "  # Create training examples / targets\n",
    "  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "  sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "  dataset = sequences.map(split_input_target)\n",
    "\n",
    "  # (TF data is designed to work with possibly infinite sequences,\n",
    "  # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "  # it maintains a buffer in which it shuffles elements).\n",
    "  dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "  return dataset, vocab, examples_per_epoch, char_map"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPGoaE2TDnX9"
   },
   "source": [
    "Now we can create and train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y5bOTe1hDqtY",
    "ExecuteTime": {
     "end_time": "2023-09-29T12:15:36.934879100Z",
     "start_time": "2023-09-29T12:15:36.933909Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Embedding(vocab_size,\n",
    "                                embedding_dim,\n",
    "                                batch_input_shape=[batch_size, None]),\n",
    "      tf.keras.layers.GRU(rnn_units,\n",
    "                          return_sequences=True,\n",
    "                          recurrent_initializer='glorot_uniform',\n",
    "                          stateful=True),\n",
    "      tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def create_model(text, epochs=3, embedding_dim=256, rnn_units=1024):\n",
    "  dataset, vocab, examples_per_epoch, char_map = prepare_text(text)\n",
    "\n",
    "  vocab_size = len(vocab)  # Length of the vocabulary in chars\n",
    "  # embedding_dim = 256  # The embedding dimension\n",
    "  # rnn_units = 1024  # Number of RNN units\n",
    "\n",
    "  model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "  # Create checkpoints once the model has been trained\n",
    "  checkpoint_dir = './training_checkpoints'\n",
    "  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_prefix,\n",
    "      save_weights_only=True)\n",
    "\n",
    "  # Train the model\n",
    "  history = model.fit(\n",
    "      dataset,\n",
    "      epochs=epochs,\n",
    "      callbacks=[checkpoint_callback])\n",
    "\n",
    "  tf.train.latest_checkpoint(checkpoint_dir)\n",
    "  model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "  model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "  model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "  return model, char_map"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tK4fcZI55rzd",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:19.317900700Z",
     "start_time": "2023-09-29T10:22:28.113945400Z"
    }
   },
   "source": [
    "shake_model, shake_chars = create_model(shakespeare)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n",
      "Epoch 1/3\n",
      "172/172 [==============================] - 214s 1s/step - loss: 2.6603\n",
      "Epoch 2/3\n",
      "172/172 [==============================] - 186s 1s/step - loss: 1.9546\n",
      "Epoch 3/3\n",
      "172/172 [==============================] - 190s 1s/step - loss: 1.6898\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_R1efC2eKH1"
   },
   "source": [
    "Now that we've trained our model, we can finally use it to generate some text. The following function takes a model and a string as input, and continually predicts and appends the next character to the string until it becomes 1,000 characters long."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KhVcs5ny-urX",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:19.324756Z",
     "start_time": "2023-09-29T10:32:19.320749600Z"
    }
   },
   "source": [
    "def generate_text(model, char_map, start_string, temperature=1.0):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  if not start_string:\n",
    "    print(\"start_string can't be empty\")\n",
    "    return \"\"\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char_map['char_to_index'][s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(char_map['index_to_char'][predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS0UlqVbhOwa"
   },
   "source": [
    "Let's generate some text!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1YOnJYAn-upC",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:25.586644900Z",
     "start_time": "2023-09-29T10:32:19.322755Z"
    }
   },
   "source": [
    "print(generate_text(shake_model, shake_chars, \"ROMEO: \", temperature=1.0))"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Indersers?\n",
      "\n",
      "MENCUTET:\n",
      "Go, sirch my bods; that se? Go the call couse, I have looks at with dier:\n",
      "Afoing Herrow!-\n",
      "LEONTES:\n",
      "Prothers; kill should so if, and risparedr nume!\n",
      "I have 'twers live,\n",
      "Arw'd lenge of yor trought thou art threazen.\n",
      "\n",
      "HESRY BOLINGBROKE:\n",
      "Hese chowery her, mut give\n",
      "With hold in.\n",
      "Dusting and mare the admigies of dares,\n",
      "Loves, or bid the nobarry pilie, har great what an well by the ne\n",
      "That him thee, Gives years, I do ass.\n",
      "\n",
      "ASTOLLO:\n",
      "Pray thou though sourte come dog.\n",
      "Or IAll this mickly parpor to enve!\n",
      "No, if a great and prince him littas thy vise!\n",
      "\n",
      "TONCESTER:\n",
      "Athis such ase med, my daughty be flemble wothim.\n",
      "\n",
      "PETRUCHIO:\n",
      "Wey, it gut shoulows from mornel!\n",
      "And resormy: but you be as? For The think or hen lime\n",
      "To a roward: thy shall Monty in theerook, which you art,\n",
      "Benest thou fless as we'll keeply parrnieve\n",
      "The devisemend. of you with you,\n",
      "stand in, morsert.\n",
      "They say that so seee, shall good deptives itseen\n",
      "With upcordleing call do as thy bose of Montious, and in the wean t\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoKGONy4fSl3"
   },
   "source": [
    "## Question 1\n",
    "The `temperature` parameter of `generate_text()`, defined earlier in the notebook, controls how predictable the generated text will be. The lower the temperature, the more the function will tend to append the most likely character (according to the model's prediction). A higher temperature introduces some randomness, leading to more unpredictable text.\n",
    "\n",
    "The text we generated above used a temperature of 1.0. Try generating more text using the Shakespeare model, once using a temperature of 0.2 and again using a temperature of 0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment\n",
    "Answer the following questions and hand in your solution in Canvas before 8:30 on Monday morning, October 2nd. Remember to save your file before uploading it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XsSNVxKEeGZb",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:44:54.545358100Z",
     "start_time": "2023-09-29T10:44:41.174732600Z"
    }
   },
   "source": [
    "# Your solution here\n",
    "print(\"\\n\\n-------------- TEMPERATURE 0.2 --------------\"\n",
    "      \"\\n\"+generate_text(shake_model, shake_chars, \"JULIET: \", temperature=0.2))\n",
    "print(\"\\n\\n-------------- TEMPERATURE 0.8 --------------\"\n",
    "      \"\\n\"+generate_text(shake_model, shake_chars, \"MERCUTIO: \", temperature=0.8))"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------- TEMPERATURE 0.2 --------------\n",
      "JULIET: I will be so see the seath of the seal.\n",
      "\n",
      "KING RICHARD III:\n",
      "A marred me so see the common of your such a manter.\n",
      "\n",
      "KING RICHARD II:\n",
      "And the seath the rest of the send the seems and the sears of my lord.\n",
      "\n",
      "KING RICHARD II:\n",
      "And there is the seems of the seave the marred of the see the seal the world be soul the dead of the seems of the rest of the servent of the dead and the common of the commons of the proud that shall not the dead and the seave the sees of the marreal of the seal.\n",
      "\n",
      "LEONTES:\n",
      "Go, sir, she shall be sone the seal the seal of the seath.\n",
      "\n",
      "KING RICHARD II:\n",
      "And there is the see the send the dear of the seed of the marred me to the prould the send the send the dead with the see the seather and the dead is a man and the dead of the seath of the world the sender of the see the dead the procest the see the dead of this with the seals of the first of the comes me the seather her for the sears the marred the seest the marred with the command.\n",
      "\n",
      "KING RICHARD II:\n",
      "Ay, sir, the see the comm\n",
      "\n",
      "\n",
      "-------------- TEMPERATURE 0.8 --------------\n",
      "MERCUTIO: then it see that thou sawest prial\n",
      "Mants, manter me stuth the drangs:\n",
      "I seer and my father, of the comms tool my forst busing that\n",
      "Romenger I will doing that seed is too me heart,\n",
      "And that with the will never it soliman.\n",
      "\n",
      "LUCIO:\n",
      "I'll take my mother with the words;\n",
      "The gites of GoLO:\n",
      "Hear not arm and in but the rine where's marry of the better\n",
      "To make extrought didcely give well.\n",
      "\n",
      "Swrets than see them wouldssticusan\n",
      "And mare tite of seast as so after.\n",
      "\n",
      "GLOUCESTER:\n",
      "In sunars, bear you be ruthmen.\n",
      "\n",
      "QUEENA:\n",
      "I discoster bear your soo?\n",
      "\n",
      "GLOUCESTER:\n",
      "I frierd ad a noble would then my sears are my lords in Grisked me?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "What soverengers death'd accourted,\n",
      "And think one that and I leave you.\n",
      "\n",
      "BONTOGK:\n",
      "Pase your reselfed dot seed with this marry be soms,\n",
      "And thates a grace langer on these are rese of York;\n",
      "And the that madsher childst and me here all take.\n",
      "They's wrong of the danger? What you not see is the pitions, made!\n",
      "Here is first that reserve who ars frind that me taunt.\n",
      "\n",
      "CO\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hauhMD0gfV2o"
   },
   "source": [
    "## Question 2\n",
    "NLTK's `names` corpus contains a list of approximately 8,000 English names. Train a new model on `names_raw` for at least 20 epochs using the `create_model(text, epochs=n)` function defined earlier. Use the trained model to generate a list of names (with the `generate_text` function defined earlier), starting with your own first name. Your name should not contain any non-English characters, and should end with an `\\n`.\n",
    "\n",
    "Print out the names that do not appear in the training data. Do you get any actual names (or at least names that sound plausible)?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NEOIqRE9fWTF",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:32:27.113550500Z",
     "start_time": "2023-09-29T10:32:25.587677600Z"
    }
   },
   "source": [
    "# Don't modify this code cell\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "nltk.download('names')\n",
    "\n",
    "# Print out a few examples\n",
    "names_raw = names.raw()\n",
    "names_unique = set(names_raw.split())\n",
    "names_raw = \"\\n\".join(names_unique)\n",
    "print(names_raw.splitlines()[:5])"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Matilda', 'Patel', 'Cayla', 'Marinna', 'Barret']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\pasqu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\names.zip.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d7Y8qE1tYR1n",
    "ExecuteTime": {
     "end_time": "2023-09-29T10:54:58.170220Z",
     "start_time": "2023-09-29T10:51:57.493299800Z"
    }
   },
   "source": [
    "# Your solution here\n",
    "names_model, names_chars = create_model(names_raw, epochs=20)\n",
    "print(generate_text(names_model, names_chars, \"Damiano\\n\", temperature=1.0))\n",
    "\n",
    "# ANSWER: It appears that the model generated names similar to plausible names, but not real names."
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 unique characters\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 9s 988ms/step - loss: 4.0577\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 3.5409\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 8s 1s/step - loss: 3.1817\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 8s 1s/step - loss: 2.9812\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 8s 1s/step - loss: 2.7682\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.5632\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.4356\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 8s 1s/step - loss: 2.3622\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.3210\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2882\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2605\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2377\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2196\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.2011\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1914\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1797\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1703\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1571\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1462\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1375\n",
      "DAMIANO\n",
      "Zovley\n",
      "Quin\n",
      "Heanaula\n",
      "Diratta\n",
      "Aimia\n",
      "Aleare\n",
      "Dorvara\n",
      "Hettina\n",
      "Chrissan\n",
      "Caldrinah\n",
      "Frely\n",
      "Caricorna\n",
      "Deo\n",
      "Commel\n",
      "Sil\n",
      "Joke\n",
      "Inerig\n",
      "Foli\n",
      "Emilaet\n",
      "Keejia\n",
      "Vrinita\n",
      "Roty\n",
      "Jelie\n",
      "Coronanche\n",
      "Mior\n",
      "Runy\n",
      "Chamery\n",
      "Moratt\n",
      "Orsa\n",
      "Dara\n",
      "Hineldaddi\n",
      "Jerheld\n",
      "Terili\n",
      "Derghibae\n",
      "Marny\n",
      "Reah\n",
      "Carlita\n",
      "Guencie\n",
      "Netal\n",
      "Eveline\n",
      "Juedor\n",
      "Labia\n",
      "Varite\n",
      "Kaunn\n",
      "Taquel\n",
      "Frabecton\n",
      "Radiand\n",
      "Orama\n",
      "Joan\n",
      "Mergiell\n",
      "Ehholina\n",
      "Janabrie\n",
      "Ehlin\n",
      "Kitran\n",
      "Gallol\n",
      "Marie\n",
      "Pethaus\n",
      "Shinth\n",
      "Gylyc\n",
      "Gantidrle\n",
      "Vid\n",
      "Jolia\n",
      "Gigra\n",
      "Shpid\n",
      "Lejaine\n",
      "Tigoma\n",
      "Lellen\n",
      "Oleltins\n",
      "Rachay\n",
      "Edrisse\n",
      "Genndetta\n",
      "Cardeol\n",
      "Gayddin\n",
      "Lansaw\n",
      "Shestoel\n",
      "Haullanda\n",
      "Chichichy\n",
      "Zhnete\n",
      "Ringa\n",
      "Rogh\n",
      "Edrie\n",
      "Romorde\n",
      "Yad\n",
      "Terelloe\n",
      "Marryn\n",
      "Celyn\n",
      "Koria\n",
      "Dlon\n",
      "Walmy\n",
      "Sadia\n",
      "Griedris\n",
      "Doway\n",
      "Donky\n",
      "Roza\n",
      "Merilin\n",
      "Hamberco\n",
      "Cledstia\n",
      "Orrid\n",
      "Zadrie\n",
      "Avianna\n",
      "Selande\n",
      "Marria\n",
      "Gordi\n",
      "Evwem\n",
      "Piqutto\n",
      "Anzany\n",
      "Belemz\n",
      "Doranda\n",
      "Anda\n",
      "Dado\n",
      "Elis\n",
      "Hary\n",
      "Gusi\n",
      "Fallika\n",
      "Gervie\n",
      "Hayk\n",
      "Marie\n",
      "Caika\n",
      "Ashetta\n",
      "Carri\n",
      "Tolera\n",
      "Anes\n",
      "Zeounsee\n",
      "Charmak\n",
      "Xynce\n",
      "Asmes\n",
      "Alellann\n",
      "Laille\n",
      "Emsin\n",
      "Parolet\n",
      "Geliadore\n",
      "Rommor\n",
      "Brerila\n",
      "Idnond\n",
      "Jaseet\n",
      "Hetoly\n",
      "Eshie\n",
      "Lylena\n",
      "Flatina\n",
      "Siphiel\n",
      "Kellia\n",
      "Sinus\n",
      "I\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Question 3\n",
    "The size of the model can make a difference when it comes to performance. Create a new model that has twice the number of hidden units as the previous model and double the size of the embeddings. How does the performance change? What happens if you decrease these parameters?"
   ],
   "metadata": {
    "id": "64h0OI83sXJE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your solution here\n",
    "names_model_2, names_chars_2 = create_model( names_raw, epochs=20, embedding_dim=512, rnn_units=2048)\n",
    "print(generate_text(names_model_2, names_chars_2, \"Damiano\\n\", temperature=1.0))\n",
    "names_model_3, names_chars_3 = create_model( names_raw, epochs=20, embedding_dim=64, rnn_units=256)\n",
    "print(generate_text(names_model_3, names_chars_3, \"Damiano\\n\", temperature=1.0))"
   ],
   "metadata": {
    "id": "Myk6_IUitYjk",
    "ExecuteTime": {
     "end_time": "2023-09-29T12:27:48.882942500Z",
     "start_time": "2023-09-29T12:15:46.774200700Z"
    }
   },
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 unique characters\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 8.1272\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 6.0124\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 3.6843\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 3.1752\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.8026\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 32s 4s/step - loss: 2.6072\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.5092\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.4557\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.4206\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 60s 7s/step - loss: 2.3874\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.3603\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.3337\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 2.3061\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 34s 4s/step - loss: 2.2863\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 34s 4s/step - loss: 2.2716\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 34s 4s/step - loss: 2.2606\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 37s 5s/step - loss: 2.2462\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 36s 5s/step - loss: 2.2343\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 34s 4s/step - loss: 2.2243\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 34s 4s/step - loss: 2.2139\n",
      "Damiano\n",
      "Cahesqueeaie-auamamaayyneeeaeaiatanaapeata-Eierearayabalieaazeaatanaaynanaleranatana\n",
      "Nantananalatataaarenatana\n",
      "Miaia\n",
      "Baniaa\n",
      "Agkaneangareaanataa\n",
      "Adaroaelaraha\n",
      "Esslyayneatardaiananadeyata\n",
      "Latta-Caranffhadaneetenaarauttatalaraamaanadaialaba\n",
      "Haudfana-Watadatatanaeeadada\n",
      "Nestajayaneethautt\n",
      "Dafapia\n",
      "Osahefffaynasatty\n",
      "Camarienkaataasaeaahafatte\n",
      "Tistaralahaylarealana\n",
      "Wiakanagana\n",
      "Estonanarananaroneetanasanattasa\n",
      "Tingeana\n",
      "Kamfaina\n",
      "Whathahana\n",
      "Batadinananarauenemy\n",
      "Naendara\n",
      "Adarayakda\n",
      "Savanabcealliak\n",
      "Liadanaaaeeiceneta\n",
      "Maydaratanamadanaphaaia\n",
      "Evanara\n",
      "Ramafetada\n",
      "Mariellia-Leraana\n",
      "Buisa\n",
      "Heardasqueladrameaop\n",
      "Caanarana\n",
      "Ignelaa\n",
      "Tatana-Janadfarstara\n",
      "Banedaaedaia\n",
      "DaMaugunasataa\n",
      "Damandatteataa\n",
      "Qudawa\n",
      "Saveg\n",
      "Caradialgaiatacsadae\n",
      "Mauga\n",
      "Cathathaeana\n",
      "Emaay\n",
      "Dameguenelea\n",
      "Wasthanani\n",
      "Ebalaba\n",
      "Granarda\n",
      "Nieenata\n",
      "Marthe\n",
      "Garkeiatala\n",
      "Finanana\n",
      "Hartanaaydayearaazaa\n",
      "Brantellaanena\n",
      "Handadey\n",
      "Maanararra\n",
      "Laatadane\n",
      "Latalama\n",
      "Jicyan-Aranayamatananatcia\n",
      "Kyaita\n",
      "Waramayna\n",
      "Wan-Candandandyarasa\n",
      "Carra\n",
      "JalKataya\n",
      "Vackea\n",
      "Arrayna\n",
      "Liagharenda\n",
      "55 unique characters\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 3.9356\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 3.5253\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 1s 169ms/step - loss: 3.3294\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 3.2090\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 3.1413\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 3.1092\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 3.0564\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 2.9853\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 2.8846\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 2.7579\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 1s 167ms/step - loss: 2.6305\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 1s 167ms/step - loss: 2.5321\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 1s 167ms/step - loss: 2.4622\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 2.4170\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 2.3888\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 2.3663\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 1s 162ms/step - loss: 2.3488\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 2.3322\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 2.3160\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 2.3023\n",
      "Damiano\n",
      "Jrabrin\n",
      "Covdy\n",
      "Masice\n",
      "Alahyl\n",
      "Teli\n",
      "Rorronne\n",
      "Lalreulhta\n",
      "Lesole\n",
      "Jinjanodl\n",
      "Erod\n",
      "Zallil\n",
      "Barrena\n",
      "Kelvan\n",
      "Prate\n",
      "Kmeta\n",
      "Derhor\n",
      "Horiatma\n",
      "Karthie\n",
      "Pollis\n",
      "Keryt\n",
      "Dafela\n",
      "Ginnogece\n",
      "Darlie\n",
      "Morlil\n",
      "Lana\n",
      "Mardytce\n",
      "Rl\n",
      "Buta\n",
      "Ginala\n",
      "Marko\n",
      "Neial\n",
      "Ildrayn\n",
      "Vital\n",
      "Corhina\n",
      "Jubgini\n",
      "Tarcs\n",
      "Tuatla\n",
      "Ruhve\n",
      "Armya\n",
      "Anl\n",
      "Kicel\n",
      "Ameli\n",
      "Culheckis\n",
      "Gispen\n",
      "Pharba\n",
      "Wedelia\n",
      "Halllin\n",
      "Kaigisedta\n",
      "Beitane\n",
      "Manene\n",
      "Euan\n",
      "Oposha\n",
      "Emtey\n",
      "Logra\n",
      "Cundy\n",
      "Belnan\n",
      "Jorrin\n",
      "Robolhla\n",
      "Deesa\n",
      "Nerisa\n",
      "Jaaves\n",
      "RogqUqUlomre\n",
      "Bysole\n",
      "Jusinl\n",
      "She\n",
      "Garetha\n",
      "Zatomoc\n",
      "Thincoa\n",
      "Arrtha\n",
      "Srmard\n",
      "Jonye\n",
      "Karlart\n",
      "Fule\n",
      "Damercane\n",
      "Heryn\n",
      "Atleshay\n",
      "Alarsede\n",
      "Macehsa\n",
      "Donnen\n",
      "Dants\n",
      "Koroiti\n",
      "Bunn\n",
      "Rargeltie\n",
      "Zeazi\n",
      "Rotie\n",
      "Erclan\n",
      "Eler\n",
      "Mance\n",
      "Bagell\n",
      "Amonnald\n",
      "Qodlra\n",
      "Daehiet\n",
      "Elon\n",
      "Ranlo\n",
      "Handara\n",
      "Ertetse\n",
      "AnnardZaa\n",
      "Fablilesa\n",
      "Rania\n",
      "Malmet\n",
      "Kely\n",
      "Leli\n",
      "Almiy\n",
      "Corit\n",
      "Kambendta\n",
      "Marera\n",
      "ParAn\n",
      "Redslli\n",
      "Lerneina\n",
      "Forinhik\n",
      "Gonille\n",
      "Selcorh\n",
      "Sitle\n",
      "Meysna\n",
      "Lilili\n",
      "Furyo\n",
      "Lo\n",
      "Perure\n",
      "Sorra\n",
      "Radce\n",
      "Lore\n",
      "Deadi\n",
      "Corgra\n",
      "Alda\n",
      "Dellien\n",
      "Aninis\n",
      "Mapel\n",
      "Heitha\n",
      "Nihhent\n",
      "Lierin\n",
      "Huis\n",
      "Wesy\n",
      "Heue\n",
      "Ceivine\n",
      "Mamino\n",
      "Zeticiephi\n",
      "Malise\n",
      "Vomuya\n",
      "Gadell\n",
      "Ka\n",
      "Mella\n",
      "Silbais\n",
      "Lwa\n",
      "G\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 4\n",
    "Transformer large language models can also generate text. The following code imports a pretrained GPT-2 model from Huggingface's Transformer library. This model can then be used directly to generate text, given a prompt as context. Alter the prompt to have the transformer model (GPT-2) generate an engaging story beginning using one of the following story starters:\n",
    "\n",
    "\n",
    "*   It was the day the moon fell.\n",
    "*   Am I in heaven?  What happened to me?\n",
    "*   Wandering through the graveyard it felt like something was watching me.\n",
    "*   Three of us.  We were the only ones left, the only ones to make it to the island.\n",
    "\n",
    "There are several different methods to choose from to generate the text (as seen in the commented out lines below). Try out the different methods and play with the parameters. This [blogpost](https://huggingface.co/blog/how-to-generate) explains their differences.\n",
    "\n",
    "Which method has the best performance?\n",
    "\n",
    "Can GPT-2 generate Shakespere?"
   ],
   "metadata": {
    "id": "2FI-1ldhn0SE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Uncomment if transformers is not installed\n",
    "!pip install transformers"
   ],
   "metadata": {
    "id": "GE-xTvvYvHAy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Do not modify this code\n",
    "# https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Today I believe we can finally\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=100) # Greedy search\n",
    "#outputs = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=3, early_stopping=True) # Beam search\n",
    "#outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=0, temperature=0.7) # Sampling\n",
    "#outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=50) # Top-k\n",
    "#outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=50, top_p=0.92) # Top-p\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ],
   "metadata": {
    "id": "2_ZMgywnnziH",
    "ExecuteTime": {
     "end_time": "2023-09-30T00:33:44.003179300Z",
     "start_time": "2023-09-30T00:33:32.583746300Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can make a difference in the lives of the people of the United States of America.\\n\\n']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Your solution here\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "\n",
    "prompts = [\n",
    "  \"It was the day the moon fell.\",\n",
    "  \"Am I in heaven?  What happened to me?\",\n",
    "  \"Wandering through the graveyard it felt like something was watching me.\",\n",
    "  \"Three of us.  We were the only ones left, the only ones to make it to the island.\"\n",
    "]\n",
    "\n",
    "def run_gpt2(prompt, method=\"greedy\"):\n",
    "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "  if method == \"greedy\":\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "  elif method == \"beam\":\n",
    "    outputs = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=3, early_stopping=True)\n",
    "  elif method == \"sampling\":\n",
    "    outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=0, temperature=0.7)\n",
    "  elif method == \"topk\":\n",
    "    outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=50)\n",
    "  elif method == \"topp\":\n",
    "    outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=50, top_p=0.92)\n",
    "  else:\n",
    "    print(\"Invalid method\")\n",
    "    return\n",
    "  print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "print(\"\\n\\n-------------- Greedy search --------------\")\n",
    "run_gpt2(prompts[3], method=\"greedy\")\n",
    "print(\"\\n\\n-------------- Beam search --------------\")\n",
    "run_gpt2(prompts[3], method=\"beam\")\n",
    "print(\"\\n\\n-------------- Sampling --------------\")\n",
    "run_gpt2(prompts[3], method=\"sampling\")\n",
    "print(\"\\n\\n-------------- Top-k --------------\")\n",
    "run_gpt2(prompts[3], method=\"topk\")\n",
    "print(\"\\n\\n-------------- Top-p --------------\")\n",
    "run_gpt2(prompts[3], method=\"topp\")\n",
    "\n",
    "print(\"\\n\\n-------------- Shakespeare - Greedy --------------\")\n",
    "run_gpt2(shakespeare[:250], method=\"greedy\")\n",
    "print(\"\\n\\n-------------- Shakespeare - Beam --------------\")\n",
    "run_gpt2(shakespeare[:250], method=\"beam\")\n",
    "print(\"\\n\\n-------------- Shakespeare - Sampling --------------\")\n",
    "run_gpt2(shakespeare[:250], method=\"sampling\")\n",
    "print(\"\\n\\n-------------- Shakespeare - Top-k --------------\")\n",
    "run_gpt2(shakespeare[:250], method=\"topk\")\n",
    "print(\"\\n\\n-------------- Shakespeare - Top-p --------------\")\n",
    "run_gpt2(shakespeare[:250], method=\"topp\")\n",
    "\n",
    "\"\"\"\n",
    "ANSWER: The best performance was achieved with the topk and topp methods. The greedy method was the worst.\n",
    "GPT-2 can generate Shakespeare, but it is not very good at it. It is able to generate some words that are in the text,\n",
    "but it is not able to generate any coherent sentences.\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "hnH32YYKraN4",
    "ExecuteTime": {
     "end_time": "2023-09-30T00:35:12.521855200Z",
     "start_time": "2023-09-30T00:34:49.150006500Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------- Greedy search --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Three of us.  We were the only ones left, the only ones to make it to the island.  We were the only ones to make it to the island.  We were the only ones to make it to the island.  We were the only ones to make it to the island.  We were the only ones to make it to the island.  We were the only ones to make it to the island.  We were the only ones to make it to the island.']\n",
      "\n",
      "\n",
      "-------------- Beam search --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Three of us.  We were the only ones left, the only ones to make it to the island.  It was the only place we could go.  And we were the ones who made it.  The only ones who could make it.\\n\\n\"I\\'m sorry.  I didn\\'t mean to hurt you.  But I don\\'t know how you feel about me.  You don\\'t deserve to be here.  That\\'s why you\\'re here.\"\\n\\nI was']\n",
      "\n",
      "\n",
      "-------------- Sampling --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Three of us.  We were the only ones left, the only ones to make it to the island. When we opened the doors at the entrance, we found the three people excluded from the festivities. \\n\"Wait a minute, we have to get lost.  Wasn\\'t there a way we could get back to the Island?\"\\n\"No, thank you very much.  I\\'ll tell you, the two of us are so far away, we\\'ll need to go back and']\n",
      "\n",
      "\n",
      "-------------- Top-k --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Three of us.  We were the only ones left, the only ones to make it to the island. I think for me that is not the type of thing that I want to be doing. But it is not only that. My friends and I were very good friends. My friends who were very good at my game were very good at chess.  They all helped me at the time. When we made it to the island and were about to go back,  they were out on the']\n",
      "\n",
      "\n",
      "-------------- Top-p --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Three of us.  We were the only ones left, the only ones to make it to the island. But then we were sent back into hiding by the Ughouls who stole our bodies in a last desperate bid to rescue you from your captivity. I never believed that you would have been able to survive alone, or at least survived without us, and I think our stories tell a similar story to those of the Others who were so willing to come to kill me. We weren't going to\"]\n",
      "\n",
      "\n",
      "-------------- Shakespeare - Greedy --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\n\\nResolved. resolved.\\n\\nFirst Citizen:\\n\\nYou are all resolved rather to die than']\n",
      "\n",
      "\n",
      "-------------- Shakespeare - Beam --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n...\\n\\nCaius:\\n\\nYes, I know, but I do not know what you mean by']\n",
      "\n",
      "\n",
      "-------------- Shakespeare - Sampling --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\n\\nResolved. resolved.\\n\\nFirst Citizen:\\n\\nYour own ignorance is your downfall.\\n']\n",
      "\n",
      "\n",
      "-------------- Shakespeare - Top-k --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\n\\nResolved. resolved.\\n\\nFirst Citizen:\\n\\nHow do you expect the Great White to']\n",
      "\n",
      "\n",
      "-------------- Shakespeare - Top-p --------------\n",
      "['First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\n\\nHow can such an individual be so foolish and so selfish as to think that he would even seek the']\n"
     ]
    }
   ]
  }
 ]
}
