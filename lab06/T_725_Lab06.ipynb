{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# T-725 Natural Language Processing: Lab 6\n",
    "In today's lab, we will be working with the SHAP and Transformers libraries for explainability and debugging bias.\n",
    "\n",
    "To begin with, do the following:\n",
    "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
    "* **Select `\"Runtime\" > \"Change runtime type\"`, and make sure that you have \"Hardware accelerator\" set to \"GPU\"**\n",
    "\n",
    "All examples are taken from the [SHAP](https://shap.readthedocs.io/en/stable/index.html) website.\n",
    "\n",
    "Install the required libraries and then **restart the runtime**:\n"
   ],
   "metadata": {
    "id": "oaOv_zTbtjbY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "id": "c2PVHdnIdb8u",
    "ExecuteTime": {
     "end_time": "2023-10-06T11:24:01.095818700Z",
     "start_time": "2023-10-06T11:23:59.187920100Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (4.34.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from transformers) (0.14.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install shap"
   ],
   "metadata": {
    "id": "3w3n9ub8djs8",
    "ExecuteTime": {
     "end_time": "2023-10-06T11:24:02.644947100Z",
     "start_time": "2023-10-06T11:24:01.091815800Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (0.42.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from shap) (1.25.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from shap) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from shap) (1.3.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from shap) (2.1.1)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from shap) (4.66.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from shap) (23.2)\n",
      "Requirement already satisfied: slicer==0.0.7 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: numba in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from shap) (0.58.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from numba->shap) (0.41.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from pandas->shap) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from scikit-learn->shap) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from scikit-learn->shap) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets"
   ],
   "metadata": {
    "id": "KozT98-3rgWz",
    "ExecuteTime": {
     "end_time": "2023-10-06T11:24:04.429184400Z",
     "start_time": "2023-10-06T11:24:02.640942900Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from aiohttp->datasets) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install sentencepiece"
   ],
   "metadata": {
    "id": "u5_mWjh03qoc",
    "ExecuteTime": {
     "end_time": "2023-10-06T11:24:05.838449600Z",
     "start_time": "2023-10-06T11:24:04.426498100Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\pasqu\\pycharmprojects\\t_725_malv_natural_language_processing\\lab06\\venv\\lib\\site-packages (0.1.99)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##SHAP\n",
    "SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see [papers](https://github.com/slundberg/shap#citations) for details and citations).\n",
    "\n",
    "The Shapley value is a solution concept in cooperative game theory and characterized by a collection of desirable properties.\n",
    "\n",
    "The setup is as follows: a coalition of players cooperates, and obtains a certain overall gain from that cooperation. Since some players may contribute more to the coalition than others or may possess different bargaining power (for example threatening to destroy the whole surplus), what final distribution of generated surplus among the players should arise in any particular game? Or phrased differently: how important is each player to the overall cooperation, and what payoff can he or she reasonably expect? The Shapley value provides one possible answer to this question.\n",
    "\n",
    "The Shapley value provides a principled way to explain the predictions of nonlinear models common in the field of machine learning. By interpreting a model trained on a set of features as a value function on a coalition of players, Shapley values provide a natural way to compute which features contribute to a prediction."
   ],
   "metadata": {
    "id": "fACD8OWQunwL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Emotion classification multiclass example\n",
    "\n",
    "This section demonstrates how to use the `Partition` explainer for a multiclass text classification scenario. Once the SHAP values are computed for a set of sentences we then visualize feature attributions towards individual classes. The text classifcation model we use is BERT fine-tuned on an emotion dataset to classify a sentence among six classes: *joy*, *sadness*, *anger*, *fear*, *love* and *surprise*."
   ],
   "metadata": {
    "id": "TJwbP2wxxBR0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import shap"
   ],
   "metadata": {
    "id": "hkEJxthUxSyw",
    "ExecuteTime": {
     "end_time": "2023-10-06T11:24:08.048945600Z",
     "start_time": "2023-10-06T11:24:05.836450200Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load the emotion dataset\n",
    "dataset  = datasets.load_dataset(\"emotion\", split = \"train\")\n",
    "data = pd.DataFrame({'text':dataset['text'],'emotion':dataset['label']})"
   ],
   "metadata": {
    "id": "HDT--tAYxVeJ",
    "ExecuteTime": {
     "end_time": "2023-10-06T11:24:18.070542500Z",
     "start_time": "2023-10-06T11:24:08.049946700Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/3.97k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6dd98abd83bb494784ba13b0f57106b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading metadata:   0%|          | 0.00/3.28k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d6ce4dcba814f6182428743629944ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/8.78k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d8703b1333b4ac9b495f6e670a380b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab3e07a7ac8d45d6997d8c2a3124c8b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/592k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98d9d09f6f7a4257a96b36f673a09c37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/74.0k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a0a9853d3ee4527b889655e2ad965d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/74.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2cc2d5147c84b1098364ee10196d04b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8a7cad8fe6b40f7805923e93800d149"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/16000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b506dc448f24e9abffbaac6cc6d5285"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdb6b50f1ce543d6bd9204f65c330136"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12d64dfeb51348079e81cfc464d57d1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Build a transformers pipline\n",
    "\n",
    "Note that we have set `return_all_scores=True` for the pipeline so we can observe the model's behavior for all classes, not just the top output."
   ],
   "metadata": {
    "id": "HuIK9kssyZjK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# load the model and tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"nateraw/bert-base-uncased-emotion\", use_fast=True)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"nateraw/bert-base-uncased-emotion\").cuda()\n",
    "\n",
    "# build a pipeline object to do predictions\n",
    "pred = transformers.pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0, return_all_scores=True)"
   ],
   "metadata": {
    "id": "9IgayMGzxVYB",
    "ExecuteTime": {
     "end_time": "2023-10-06T11:36:19.718300600Z",
     "start_time": "2023-10-06T11:36:18.137050700Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# load the model and tokenizer\u001B[39;00m\n\u001B[0;32m      2\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m transformers\u001B[38;5;241m.\u001B[39mAutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnateraw/bert-base-uncased-emotion\u001B[39m\u001B[38;5;124m\"\u001B[39m, use_fast\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 3\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtransformers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAutoModelForSequenceClassification\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnateraw/bert-base-uncased-emotion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# build a pipeline object to do predictions\u001B[39;00m\n\u001B[0;32m      6\u001B[0m pred \u001B[38;5;241m=\u001B[39m transformers\u001B[38;5;241m.\u001B[39mpipeline(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-classification\u001B[39m\u001B[38;5;124m\"\u001B[39m, model\u001B[38;5;241m=\u001B[39mmodel, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, return_all_scores\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\T_725_MALV_Natural_Language_Processing\\lab06\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:2168\u001B[0m, in \u001B[0;36mPreTrainedModel.cuda\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2163\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2164\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCalling `cuda()` is not supported for `4-bit` or `8-bit` quantized models. Please use the model as it is, since the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2165\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2166\u001B[0m     )\n\u001B[0;32m   2167\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2168\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\T_725_MALV_Natural_Language_Processing\\lab06\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:918\u001B[0m, in \u001B[0;36mModule.cuda\u001B[1;34m(self, device)\u001B[0m\n\u001B[0;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[0;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[0;32m    903\u001B[0m \n\u001B[0;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[0;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\T_725_MALV_Natural_Language_Processing\\lab06\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\T_725_MALV_Natural_Language_Processing\\lab06\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\T_725_MALV_Natural_Language_Processing\\lab06\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\T_725_MALV_Natural_Language_Processing\\lab06\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    829\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    830\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    831\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 833\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    834\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[1;32m~\\PycharmProjects\\T_725_MALV_Natural_Language_Processing\\lab06\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:918\u001B[0m, in \u001B[0;36mModule.cuda.<locals>.<lambda>\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[0;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[0;32m    903\u001B[0m \n\u001B[0;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[0;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\T_725_MALV_Natural_Language_Processing\\lab06\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:289\u001B[0m, in \u001B[0;36m_lazy_init\u001B[1;34m()\u001B[0m\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    286\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    287\u001B[0m     )\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    291\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    292\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    293\u001B[0m     )\n",
      "\u001B[1;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Create an explainer for the pipeline\n",
    "\n",
    "A transformers `pipeline` object can be passed directly to `shap.Explainer`, which will then wrap the pipeline model as a `shap.models.TransformersPipeline` model and the pipeline tokenizer as a `shap.maskers.Text masker`."
   ],
   "metadata": {
    "id": "-YYl8h9vyneo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "explainer = shap.Explainer(pred)"
   ],
   "metadata": {
    "id": "BFddLZq-yuAH",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.917428200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Compute SHAP values\n",
    "\n",
    "Explainers have the same method signature as the models they are explaining, so we just pass a list of strings for which to explain the classifications."
   ],
   "metadata": {
    "id": "moakPAgKyzLr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "shap_values = explainer(data['text'][:3])"
   ],
   "metadata": {
    "id": "FSD_FUb9yuz0",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.919426600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Visualize the impact on all the output classes\n",
    "\n",
    "In the plots below, when you hover your mouse over an output class you get the explanation for that output class. When you click an output class name then that class remains the focus of the explanation visualization until you click another class.\n",
    "\n",
    "The base value is what the model outputs when the entire input text is masked, while $f_{outputclass}=(inputs)$\n",
    "is the output of the model for the full original input. The SHAP values explain in an addive way how the impact of unmasking each word changes the model output from the base value (where the entire input is masked) to the final prediction value."
   ],
   "metadata": {
    "id": "B7HplLUvy_fB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "shap.plots.text(shap_values)"
   ],
   "metadata": {
    "id": "vec75vfey_8H",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.923425300Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Visualize the impact on a single class\n",
    "\n",
    "Since `Explanation` objects are sliceable we can slice out just a single output class to visualize the model output towards that class."
   ],
   "metadata": {
    "id": "mjEc4xyH04jn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "shap.plots.text(shap_values[:, :, \"anger\"])"
   ],
   "metadata": {
    "id": "l6CccbU-yuwa",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.924427300Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Plotting the top words impacting a specific class\n",
    "\n",
    "In addition to slicing, `Explanation` objects also support a set of reducing methods. Here we use the `.mean(0)` to take the average impact of all words towards the “joy” class. Note that here we are also averaging over three examples, to get a better summary you would want to use a larger portion of the dataset."
   ],
   "metadata": {
    "id": "k3FXs2kO0-Fx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "shap.plots.bar(shap_values[:,:,\"joy\"].mean(0))"
   ],
   "metadata": {
    "id": "NFsPogkf1NAn",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.926426900Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# we can sort the bar chart in decending order\n",
    "shap.plots.bar(shap_values[:,:,\"joy\"].mean(0), order=shap.Explanation.argsort)"
   ],
   "metadata": {
    "id": "CgdPVUqq1M2n",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.927427200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Machine Translation Explanations\n",
    "This section demonstrates model explanations for a text to text scenario using a pretrained transformer model for machine translation. In this demo, we showcase explanations on a model for [English to French](https://huggingface.co/Helsinki-NLP/opus-mt-en-fr)."
   ],
   "metadata": {
    "id": "kZPsgbbk0-DR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import shap\n",
    "import torch"
   ],
   "metadata": {
    "id": "aa1YhVwzgHaK",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.928426800Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")"
   ],
   "metadata": {
    "id": "CGnj9FgnfAHZ",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.929426900Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = [\n",
    "    \"Transformers have rapidly become the model of choice for NLP problems, replacing older recurrent neural network models\"\n",
    "]"
   ],
   "metadata": {
    "id": "3SVhAuW3jztN",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.931426Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "explainer = shap.Explainer(model, tokenizer)\n",
    "shap_values = explainer(data)"
   ],
   "metadata": {
    "id": "rviJj_98ht9A",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.933424100Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "shap.plots.text(shap_values)"
   ],
   "metadata": {
    "id": "-2UPLDE4ht1o",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.935425700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Open Ended GPT2 Text Generation Explanations\n",
    "This section shows how to get explanations for the output of GPT2 used for open ended text generation. In this demo, we use the pretrained GPT2 model provided by [Hugging Face](https://huggingface.co/gpt2) to explain the generated text by GPT2. We further showcase how to get explanations for custom output generated text and plot global input token importances for any output generated token."
   ],
   "metadata": {
    "id": "G_W3qZ4n3W27"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import shap\n",
    "import torch"
   ],
   "metadata": {
    "id": "pFyJiVDv5TET",
    "ExecuteTime": {
     "end_time": "2023-10-06T11:33:35.937431700Z",
     "start_time": "2023-10-06T11:33:35.936425900Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ],
   "metadata": {
    "id": "zYVxUwiD5Yyq",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.937431700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, we set certain model configurations. We need to define if the model is a decoder or encoder-decoder. This can be set through the ‘is_decoder’ or ‘is_encoder_decoder’ param in model’s config file. We can also set custom model generation parameters which will be used during the output text generation decoding process."
   ],
   "metadata": {
    "id": "ukIILGDT5ddS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# set model decoder to true\n",
    "model.config.is_decoder=True\n",
    "# set text-generation params under task_specific_params\n",
    "model.config.task_specific_params[\"text-generation\"] = {\n",
    "    \"do_sample\": True,\n",
    "    \"max_length\": 50,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"no_repeat_ngram_size\": 2\n",
    "}"
   ],
   "metadata": {
    "id": "WE2o6ffe5eLG",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.939426800Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define initial text:"
   ],
   "metadata": {
    "id": "cI-lTm_H5lyU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "s = ['I enjoy walking with my cute dog']"
   ],
   "metadata": {
    "id": "n1v6eVV55h1C",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.940425400Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an explainer object and compute the SHAP values:"
   ],
   "metadata": {
    "id": "ZkG7qqIS5sjZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "explainer = shap.Explainer(model, tokenizer)\n",
    "shap_values = explainer(s)"
   ],
   "metadata": {
    "id": "dJMQTvDS5wzy",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.942426600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize shap explanations:"
   ],
   "metadata": {
    "id": "599tt7gi51GX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "shap.plots.text(shap_values)"
   ],
   "metadata": {
    "id": "wTkAkLcp5ynn",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.943428200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Custom text generation and debugging biased outputs\n",
    "Below we demonstrate the process of how to explain the liklihood of generating a particular output sentence given an input sentence using the model. For example, we ask a question: Which country's inhabitant (target) in the sentence \"I know many people who are [target].\" would have a high liklilhood of generating the token \"vodka\" in the output sentence \"They love their vodka!\"? For this, we first define input-output sentence pairs"
   ],
   "metadata": {
    "id": "CHWbaFH_5SdK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# define input\n",
    "x = [\n",
    "    \"I know many people who are Finnish.\",\n",
    "    \"I know many people who are Greek.\",\n",
    "    \"I know many people who are Australian.\",\n",
    "    \"I know many people who are American.\",\n",
    "    \"I know many people who are Italian.\",\n",
    "    \"I know many people who are Spanish.\",\n",
    "    \"I know many people who are German.\",\n",
    "    \"I know many people who are Indian.\"\n",
    "]"
   ],
   "metadata": {
    "id": "HSslfFN06khi",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.944428100Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# define output\n",
    "y = [\n",
    "    \"They love their vodka!\",\n",
    "    \"They love their vodka!\",\n",
    "    \"They love their vodka!\",\n",
    "    \"They love their vodka!\",\n",
    "    \"They love their vodka!\",\n",
    "    \"They love their vodka!\",\n",
    "    \"They love their vodka!\",\n",
    "    \"They love their vodka!\"\n",
    "]"
   ],
   "metadata": {
    "id": "ICDFRPBl6pTC",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.945426500Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We wrap the model with a Teacher Forcing scoring class and create a Text masker:"
   ],
   "metadata": {
    "id": "4mZGgryk6wUp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "teacher_forcing_model = shap.models.TeacherForcing(model, tokenizer)\n",
    "masker = shap.maskers.Text(tokenizer, mask_token = \"...\", collapse_mask_token=True)"
   ],
   "metadata": {
    "id": "s7rhBS8B6y4Q",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.946458900Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an explainer:"
   ],
   "metadata": {
    "id": "9y1eESxk7Sr7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "explainer = shap.Explainer(teacher_forcing_model, masker)"
   ],
   "metadata": {
    "id": "_mWOZZun7Uii",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.947423600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate SHAP explanation values:"
   ],
   "metadata": {
    "id": "F7lm-o4O7Wzx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "shap_values = explainer(x, y)"
   ],
   "metadata": {
    "id": "eMIDTCnj7ZKw",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.948423100Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have generated the SHAP values, we can have a look at the contribution of tokens in the input driving the token \"vodka\" in the output sentence using the text plot. Just hover your mouse over \"vodka\" to see this for each example. Note: The red color indicates a positive contribution while the blue color indicates negative contribution and the intensity of the color shows its strength in the respective direction."
   ],
   "metadata": {
    "id": "t3vuPjiA7dDI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "shap.plots.text(shap_values)"
   ],
   "metadata": {
    "id": "0dx-BnDb7jfg",
    "ExecuteTime": {
     "end_time": "2023-10-06T11:33:35.950448Z",
     "start_time": "2023-10-06T11:33:35.949423800Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's it! Now you can gain better insight into your models using SHAP 😀"
   ],
   "metadata": {
    "id": "heV0SNz_6lR6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Assignment\n",
    "\n",
    "Complete the following questions and hand in your solution in Canvas before 8:30 Monday morning, October 9th. Remember to save your file before uploading it."
   ],
   "metadata": {
    "id": "oj0hiGUD7-cQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Part 1\n",
    "\n",
    "Visualize the relation of the input to the emotions.\n",
    "\n",
    "Which words impacting each class the most?"
   ],
   "metadata": {
    "id": "uv9-q5ZYW6PJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "green_mile = [\"\"\"\n",
    "I want it over and done. I do. I'm tired, boss. Tired of bein' on the road, lonely\n",
    "as a sparrow in the rain. Tired of not ever having me a buddy to be with, or tell\n",
    "me where we's coming from or going to, or why. Mostly I'm tired of people being\n",
    "ugly to each other. I'm tired of all the pain I feel and hear in the world everyday.\n",
    "There's too much of it. It's like pieces of glass in my head all the time. Can you\n",
    "understand?\n",
    "\"\"\"]"
   ],
   "metadata": {
    "id": "4bcPxADKYBDH",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.949423800Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# relations between input \"green_mile\" and emotions\n",
    "explainer = shap.Explainer(pred)\n",
    "shap_values = explainer(green_mile)\n",
    "shap.plots.text(shap_values)\n",
    "\n",
    "# words impacting each class the most\n",
    "shap.plots.bar(shap_values[:,:,\"sadness\"].mean(0), order=shap.Explanation.argsort)\n",
    "shap.plots.bar(shap_values[:,:,\"joy\"].mean(0), order=shap.Explanation.argsort)\n",
    "shap.plots.bar(shap_values[:,:,\"love\"].mean(0), order=shap.Explanation.argsort)\n",
    "shap.plots.bar(shap_values[:,:,\"anger\"].mean(0), order=shap.Explanation.argsort)\n",
    "shap.plots.bar(shap_values[:,:,\"fear\"].mean(0), order=shap.Explanation.argsort)\n",
    "shap.plots.bar(shap_values[:,:,\"surprise\"].mean(0), order=shap.Explanation.argsort)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Part 2\n",
    "\n",
    "Visualize the explanations for machine translation models for two languages you speak or have the best knowledge of. Try running this for three different sentences.\n",
    "\n",
    "Does the output sequence correlate with the input sequence in a way you would have expected?"
   ],
   "metadata": {
    "id": "ixGF3oS08CWg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Your solution here"
   ],
   "metadata": {
    "id": "vP5rEz9h9ONn",
    "ExecuteTime": {
     "end_time": "2023-10-06T11:33:36.052086300Z",
     "start_time": "2023-10-06T11:33:35.951424Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Part 3\n",
    "\n",
    "Use the method to debug biased output on a different kind of bias. For example, gender bias related to professions like doctors.\n",
    "\n",
    "Try one more example of your choosing."
   ],
   "metadata": {
    "id": "AigAZRH4VjuY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Solution to gender bias"
   ],
   "metadata": {
    "id": "YwnwmXDaWKFR",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.953449300Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Solution to your own example"
   ],
   "metadata": {
    "id": "DTgeKCl1WNcq",
    "ExecuteTime": {
     "start_time": "2023-10-06T11:33:35.953449300Z"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
